\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\usepackage{url}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{abbrev_reactive}
\usepackage{listings}%[hyper,procnames]
\lstset{extendedchars=true, language=C++, basicstyle=\scriptsize\ttfamily, numbers=left,
  numberstyle=\tiny, stepnumber=5, numberfirstline=true,
  tabsize=8, tab=\rightarrowfill, keywordstyle=\bf,
  stringstyle=\rmfamily, commentstyle=\rmfamily\itshape}

\let\OldRightarrow=\Rightarrow
\RequirePackage{marvosym}
\let\MarvosymRightarrow=\Rightarrow
\let\Rightarrow=\OldRightarrow
\RequirePackage{wasysym}
\let\Lightning\UnTrucIndefini% Because conflict between ifsym and marvosym
\let\Sun\UnTrucIndefini%
\RequirePackage[weather]{ifsym}


\sloppy

\begin{document}

\title{Par4All \& PIPS programming rules}

\author{Corinne \textsc{Ancourt} \and Béatrice \textsc{Creusillet} \and
  François \textsc{Irigoin} \and Ronan \textsc{Keryell}\\
  ---\\
  HPC Project \& Mines ParisTech}

\maketitle

\tableofcontents{}

% To automatically build reports from this content:
%%ContentBegin


\section{Introduction}
\label{sec:introduction}

Automatic parallelization is an intractable problem in the general
case and it would be an illusion to rely on it in any
case. Fortunately, \emph{well written} sequential programs can expose
enough parallelism, in such a way automatic parallelizers can do a
pretty good job. In particular, we advocate that, before any attempt
at manual or automatic parallelization, programs should be cleared of
specific sequential processor optimizations.

Here we present some coding rules specific to the \Apfa automatic
parallelizer that is based on the \Apips source-to-source language
transformation workbench. But we think that these coding rules can be
useful with other tools as interesting guidelines too.

This document gives advices to write programs that are compliant with the
current state of \Apips and that maximize the precision of analyses and
thus help the tool in generating efficient code. As a preliminary clue,
and not surprisingly, let us say that the more structured a program is,
the easier it is to analyze.


\section{C language}
\label{sec:c-language}

In the following we assume the reader is knowledgeable of the C99
programming language and more specifically the ISO/IEC 9899:TC3 WG14
standard\footnote{The last draft can be found here:
  \url{http://www.open-std.org/JTC1/SC22/WG14/www/docs/n1256.pdf}}.


\subsection{Program structure}
\label{sec:program-structure}

\Apips internal representation of a function body is a hierarchical
control flow graph, which simply means that syntactic constructs fit
into each other as Russian dolls. For instance, a \texttt{for} loop is
made of a header, itself made of three expressions, and a body, which
can itself include other loops, tests\ldots{} Most analyses and
transformations rely on the good properties of these constructs to
generate better results. On the contrary, a poorly structured program
(which contains \texttt{goto}s, \texttt{break}s, multiple {return}s
\ldots) leads to poor results and may prevent further code
transformations.

\Apips includes a code restructurer, but it has its own limitations. So a
good programming practice is to avoid non-structured code. Sometimes
however it may not be possible. In particular, this is the case when
testing system calls return values. But, however important these tests
are, they do not affect the program semantics when there is a correct
execution, and they are sometimes implemented differently on the targeted
architectures. So a good pratice would be to wrap system calls in
dedicated macros or functions which would receive a simpler and
non-blocking definition for \Apips, different from the real implementation
that can be more complex. Figure~\ref{fig:wrapping_system_calls} gives an
exemple for the \texttt{malloc()} function.

\begin{figure}
\begin{lstlisting}
// malloc wrapper with error testing
void * my_malloc(size_t size, int err_code,
                 const char * restrict err_mess)
{
  void * res = malloc(size);
  if (res == NULL) {
    // This IO is an issue since it prevent parallelization:
    fprinf(stderr, err_mess);
    /* This exit is transformed as an unstructured control graph, reducing
       parallelization opportunities... */
    exit(err_code);
  }
  return(res);
}

/* wrapper provided to PIPS. Note that it is not this function that has to
   be used in the compiled code by the backend */
void * my_malloc(size_t size, int err_code,
                 const char * restrict err_mess)
{
  // No more IO or exit()!
  void * res = malloc(size);
  return(res);
}
\end{lstlisting}

  \caption{Wrapping system calls.}
  \label{fig:wrapping_system_calls}
\end{figure}

Another solution is to wrap error code between %
\lstinline|#ifdef ERROR_CONTROL ... #endif| pairs to be able to
remove such code on demand.


Traditionally, automatic parallelizers focus on \texttt{for} loops
with good properties, that is loops which are similar to Fortran
\texttt{do} loops: an integer loop index which is initialized before
entering the loop, and the value of which is incremented and compared
with loop-invariant values at each iteration. \Apips front end
performs the detection of such loops from C \texttt{for} and
\texttt{while} loops, and the parallelizing effort exclusively focuses
on them. It is thus of a tremendous importance to write these loops in
such a way that they can be detected as Fortran-like \texttt{do}
loops. In particular, this implies using simple comparison and
increment expressions, and avoiding putting the \texttt{for} body code
inside the increment expression as in:
\begin{lstlisting}
for (i=0; i<n; a[i] = i, i++);
\end{lstlisting}


One of the main other restrictions is that \Apips does not handle
recursive functions.


\subsection{Declaration scope}
\label{sec:declaration-scope}

C99 offers the possibility of declaring data almost anywhere in the
code. This feature is relatively new in \Apips and phases are
unequally ready to accept this feature, which means that you may
gather all data declarations at the beginning of a code block. This is
particularly true when declarations are intermingled with jumping
constructs.

\Apips analyses are interprocedural and can deal with parameter
passing through global variables, or with static local
variables. However, using global declarations may induce memory
dependences which prevent parallelization. Program transformations
(scalar and array privatization, array scalarization) which could
remove some of these dependences, currently (and safely) do not deal
with global variables. This would require interprocedural program
transformations which are not yet available.

In addition, the current generation of \Agpu code may choke on some
case of global arrays that are not well outlined and forgotten.

As a consequence, such global declarations should be avoided if
possible and replaced by explicit argument passing in functions.



\subsection{Data Structures}
\label{sec:data_structures}

The C language provides several basic data structures, which can be
combined to create more complicated ones. \Apips allows all
combinations, but with some limitations in their usage. Notice however
that unions are not currently well analysed and must be avoided, as
well as pointers towards functions, and recursive data structures.


\subsection{Pointers}
\label{sec:pointers}

Pointers allow to designate a single memory location using several
names: this is called aliasing. However practical it may be, it may
render subsequent program analyses more complicated. To generate safe
results, the default behaviour of \Apips is to generate imprecise
results in case of pointer dereferencing. If you know that you very
safely use pointers, you can turn off this default behaviour, by
setting the right properties (see the \texttt{pipsmake-rc} guide for
more details). \Apfa, as a pragmatic tool, can do this right for you,
if you specify the \texttt{--no-pointer-aliasing} command-line option.

A pointer analysis phase, called \emph{Pointer Values}, is currently being
integrated to \Apips to avoid these drawbacks\footnote{Expert users can
  use it to check a function pointer usage. For example in \Apips by
  activating the \texttt{PRINT\_CODE\_POINTER\_VALUES} and displaying the
  \texttt{PRINTED\_FILE} resource.}.

But if you want to make the most of \Apfa, just follow the next rules:
\begin{itemize}
\item always use the same expression to refer to a memory location
  inside a function; for instance, avoid the following kind of code:
  \begin{lstlisting}
    parts = my_syst->domain[i]->parts;
    parts[j] = ...;
  \end{lstlisting}
  since \lstinline|my_syst->domain[i]->parts| is designated by two
  different expressions. This limitation does not hold for array
  index: \lstinline|my_syst->domain[i]| and
  \lstinline|my_syst->domain[j]| are rightly distinguished if
  $\mathtt{i} \neq \mathtt{j}$.
\item do not assign two different memory locations to a pointer (that
  means that a pointer could be considered rather as single assignment
  variable);
\item in particular, do not use pointer arithmetic;
\item reserve the use of pointers for the sole dynamic allocation of
  arrays and to function parameter passing;
\item avoid function pointers, they currently lead to imprecise results.
\item do not use recursive data structures such as linked lists, trees\ldots
\end{itemize}
Several of these limitations will be removed in a near future, so stay tuned!

For example avoid some code that use pointers to do some strength
reduction on array accesses such as:
\begin{lstlisting}
double a[N], b[N];
double *s = a, *d = b;
for(int i = 0; i < N; i++)
  *d++ = *s++;
\end{lstlisting}
but use a clearer version that reflect the original algorithm (or that can
be seen as an inductive variable detection on the previous code):
\begin{lstlisting}
double a[N], b[N];
for(int i = 0; i < N; i++)
  b[i] = a[i];
\end{lstlisting}
which is detected as a parallel loop!

\subsection{Arrays}
\label{sec:arrays}

\subsubsection{Declarations}
\label{sec:declarations}

As array manipulations are often the source of massive parallelism, \Apips
program transformations rely on powerful analyses (convex array region
analyses) of array element flows over the whole program.

These analyses currently assume that all array usages conform to the
declarations (no array bound check is performed). Note that you can have
variable size arrays in C99 (as available in Fortran for decades), such
as:
\begin{lstlisting}
int n = f();
int m = g();
double a[3*n][m+7];
\end{lstlisting}
That removes many older linearization old cases as seen later.


\subsubsection{References to array elements}
\label{sec:refer-array-elem}

Since array reference are represented in \Apips by using integer
polyhedron lattice, array references should be affine so that the
parallelizer can prove iteration independence. For example
\lstinline|a[2*i-3+m][3*i-j+6*n]| is an affine array reference but
\lstinline|a[2*i*j][m*n-i+j]| is not (it is a polynomial of several
variables).

This explain why you should not use array linearization to emulate
access to multidimensional arrays as one-dimensional arrays, such as:
\begin{lstlisting}
double a[n][m][l];
double * p = a;
for(int i = 0; i < n; i++)
  for(int j = 0; j < m; j++)
    for(int k = 0; k < l; k++)
      p[m*l*i + l*j + k] = ...;
\end{lstlisting}
instead of the cleaner understandable version:
\begin{lstlisting}
double a[n][m][l];
for(int i = 0; i < n; i++)
  for(int j = 0; j < m; j++)
    for(int k = 0; k < l; k++)
      a[i][j][k] = ...;
\end{lstlisting}
In the first cluttered version we have a polynomial expression which can
not represented in the linear algebra framework of \Apips and this kind of
loops can not be parallelized and communications on \Agpu cannot be
generated.

The following version:
\begin{lstlisting}
double a[n][m][l];
double * p = a;
for(int i = 0; i < n*m*l; i++)
  p[i] = ...;
\end{lstlisting}
can be parallelized in the \Aopenmp output but the communications cannot
be generated in the \Agpu version yet. So avoid array linearization if
possible.

In the future, we may implement transformations that delinearize this kind
of array accesses.

\begin{figure}
  \centering
  \begin{lstlisting}
/* In the following statement sequences and expression, all reference to a
   and b are made in a following way. So first the region with a[i-1] and
   a[i] is built, which is compact, and the a[i+1] reference is added,
   leading still to a compact region. */
tmp = a[i-1] + a[i] + a[i+1];
b[i-1] = ...;
b[i]   = ...;
b[i+1] = ...;
  \end{lstlisting}
  \caption{Consecutive array accesses.}
  \label{fig:consecutive_array_accesses}
\end{figure}

\begin{figure}
  \centering
  \begin{lstlisting}
/* The array region access is first computed from a[i-1] and a[i+1] which
   is non compact and then the a[i] element is added */
tmp = a[i-1] + a[i+1] + a[i];
/* The same for b */
b[i]   = ...;
b[i+1] = ...;
b[i-1] = ...;
  \end{lstlisting}
  \caption{Disjoint array accesses leading to imprecise array region
    analysis.}
  \label{fig:disjoint_array_accesses}
\end{figure}

To reduce their complexity, these analyses gather array elements in
sets represented by convex polyhedra. This means that non-convex sets
of array elements are approximated by sets that contain elements which
do not belong to the actual set, and are thus imprecise. Of course,
further analyses and transformations are more likely to succeed and
produce efficient code if array region analyses are more precise. So,
in case of successive accesses to array elements, it is recommended to
group them as much as possible so that two consecutive accesses in the
program flow can be represented by a convex set. As an example, you
will prefer the version of figure~\ref{fig:consecutive_array_accesses}
to the version of figure~\ref{fig:disjoint_array_accesses}.




\section{Operators and standard libraries}
\label{sec:operators_and_standard_libraries}

A huge effort has been made to integrate all standard libraries
functions. However, depending on your operating system and/or your
compiler version, some of your favorite ones may be missing. Don't
hesitate to contact us on \texttt{pipsdev at cri dot ensmp dot fr} so
that we can integrate them.

The effects of the cast operators on the analyses is currently under
study: please use it sparingly.

\section{Values of scalar variables}
\label{sec:values_of_scalar_variables}

\Apips contains powerful semantics analysis of scalar variables values
whose results are used by many other program analyses and
transformations, such as dead code elimination, parallelization,
array region analyses \ldots

To help these analyses, you should avoid embedding key scalar values
involved in loop bounds, test conditions and array sizes and index
expressions inside structured data types.

Also if you need flip-flop values inside a for loop, prefer the code
of Listing~\ref{fig:good_flip_flop} to the code of
Lising~\ref{fig:bad_flip_flop}.

  \begin{figure}
    \begin{lstlisting}
  val1 = 0;
  val2 = 1;
  for(i=0; i<n; i++)
  {
     ...
     val1 = val2;
     val2 = 1-val1;
  }
    \end{lstlisting}
    \caption{Don't use this flip-flop coding style}
    \label{fig:bad_flip_flop}
  \end{figure}
  \begin{figure}
    \begin{lstlisting}
  for(i=0; i<n; i++)
  {
     val1 = i%2;
     val2 = 1-val1;
     ...
  }
    \end{lstlisting}
    \caption{Prefer this flip-flop coding style}
    \label{fig:good_flip_flop}
  \end{figure}

\section{Fortran language}
\label{sec:fortran-language}


\section{Directives (OpenMP...)}
\label{sec:directives}


\section{Using libraries}
\label{sec:using-libraries}

FFTW

Can be seen as a DSeL


\section{Conclusion}
\label{sec:conclusion}

\Apips/\Apfa is a powerful tool to generate code for a variety
of architectures. To get the most of it, you should however follow
some rules when building your application.

The latter should be as structured as possible, and use
\lstinline{for} loops like Fortran \lstinline|do| loops preferably to
\lstinline{while} loops. Loop nest candidates for parallelization should
not contain IOs or debug or error control code, or provide an easy way
to switch them off.

Minimize the use of pointers as explained before, and don't use
recursive data structures and function pointers. Then use your data as they are
declared: in particular avoid array linearization and casting.

If some of these recommendations cannot be followed, then try to group
the non-compliant code outside of loop nests which are good
candidates for parallelization.  This is a good practice, for instance,
for heap allocations.

Even if these programming rules seem constraining, they are often
considered as sound programming rules even for classical sequential
programming. That means that (re)writing application to ease
parallelization can be a good opportunity for code cleaning and
modernization, independently of \Apips or \Apfa.


%%ContentEnd

\end{document}

do loops => simple for, without complex expressions -> done (BC, 2012-05-02)

Keep the algorithmic attitude non offuscated contest


#pragma

No union -> added (BC, 2012-05-02)

comments

cast

compléter linearization

complete Operators and standard librarie with stubs

reductions

Declarations of arrays with/without malloc.

passing arrays to functions

Commentaires de François :
* pas d'appels recursifs (c'est actuellement detecte a posteriori par
pipsmake)

* pas d'arithmetique sur pointeurs

* pas d'affectation multiples a un pointeur (ca doit faire sauter les
codes exploitant des structures de donnees recursives); pour Ter@ops:
assignation dynamique unique a chaque execution de la fonction

* pas de tableaux de pointeurs

* pas de pointeurs sur des fonctions -> added (BC, 2012-05-02)

* verification des intrinsics utilises: je ne sais comment traiter ce
probleme

* accepter ou non des subscript expressions non-affines?

* absence d'aliasing entre parametres formels "a la Fortran" (je ne vois
pas comment on pourrait verifier ca... sans faire d'analyse(s)
compliquees; voir these de Nga Nguyen)

* restriction sur l'utilisation directe et indirecte des variables
globales?

* pas de cast: "p = (void *) q;" par exemple

* pas d'unsigned?

* proscrire les effets de bord dans les expressions, y compris les
conditions (ca evite les expressions hors norme comme i + i++)

* ...

Suggestions d'outils touvees dans la doc ter@ops: Artistic Style et Doxygen

Autre suggestion: repartir de regles de codage existantes comme CERT C
plutot que d'essayer de redefinir un nouveau jeu de regles?


%%% Local Variables:
%%% mode: latex
%%% ispell-local-dictionary: "american"
%%% End:

