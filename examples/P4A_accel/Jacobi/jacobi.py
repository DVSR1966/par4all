# This is a small example showing how to tweak the communication location
# to lower the communication between the host and the GPU

from pyps import *
import re

program = "jacobi"

# Just in case it existed before:
workspace.delete(program)

# Use a special preprocessor to track #include:
os.environ['PIPS_CPP'] = 'p4a_recover_includes --simple -E'

w = workspace([ program + ".c",
		# Add also stubs for the run time definition:
		os.path.join(os.environ["P4A_ACCEL_DIR"], "p4a_stubs.c") ],
	      name = program,
	      # Ask for precize analysis:
	      activates = [ "TRANSFORMERS_INTER_FULL",
			    "INTERPROCEDURAL_SUMMARY_PRECONDITION",
			    "PRECONDITIONS_INTER_FULL" ],
	      verboseon=True)

w.set_property(
	# Useless to go on if something went wrong:
	ABORT_ON_USER_ERROR = True,
	PRETTYPRINT_STATEMENT_NUMBER = True,
	# Compute the intraprocedural preconditions at the same time as
	# transformers and use them to improve the accuracy of expression
	# and statement transformers:
	SEMANTICS_COMPUTE_TRANSFORMERS_IN_CONTEXT = True,
	# Use the more precise fix point operator to cope with while loops:
	SEMANTICS_FIX_POINT_OPERATOR = "derivative",
	# Try to restructure the code for more precision:
	UNSPAGHETTIFY_TEST_RESTRUCTURING = True,
	UNSPAGHETTIFY_RECURSIVE_DECOMPOSITION = True,
	# Simplify for loops into Fortran do-loops internally for better
	# precision of analysis:
	FOR_TO_DO_LOOP_IN_CONTROLIZER = True,
	# Regions are a must! :-) Ask for most precise regions:
	MUST_REGIONS = True,
	# Warning: assume that there is no aliasing between IO
	# streams ('FILE *' variables):
	ALIASING_ACROSS_IO_STREAMS = False,
	# Warning: this is a work in progress. Assume no weird
	# aliasing
	CONSTANT_PATH_EFFECTS = False
	)

# For verbosity, show what we've parsed:
w.all_functions.display(PRETTYPRINT_SEQUENTIAL_STYLE = "do")

# Skip the compilation units and the modules of P4A runtime, they are just
# here so that PIPS has a global view of what is going on, not to be
# parallelized :-)
skip_p4a_runtime_and_compilation_unit_re = re.compile("P4A_.*|.*!")
def is_not_p4a_runtime(module):
	#print module.name
	return not skip_p4a_runtime_and_compilation_unit_re.match(module.name)

# The modules but the P4A runtime:
mn = w.filter(is_not_p4a_runtime)

#for i in mn:
#	print i.name

mn.loop_normalize(
	# Loop normalize for the C language and GPU friendly
	LOOP_NORMALIZE_ONE_INCREMENT = True,
	# Arrays start at 0 in C, so the iteration loops:
	LOOP_NORMALIZE_LOWER_BOUND = 0,
	# It is legal in the following by construction (...Hmmm to verify)
	LOOP_NORMALIZE_SKIP_INDEX_SIDE_EFFECT = True)

mn.privatize_module()

mn.display(With="PRINT_CODE_REGIONS")


# mn.localize_declaration()

# mn.display(With="PRINT_CODE_PRECONDITIONS")

# Do the parallelization work:
mn.coarse_grain_parallelization()

# Some verification output:
mn.display()

# First, only generate the launchers to work on them later. They are
# generated by outlining parallel loops
mn.gpu_ify(GPU_USE_WRAPPER = False,
	   GPU_USE_KERNEL = False)

# Some verification output:
mn.display()

# Isolate kernels by using the fact that all the generated kernels have
# their name beginning with "p4a_":
kernel_launcher_filter_re = re.compile("p4a_kernel_launcher_.*[^!]$")
kernel_launchers = w.filter(lambda m: kernel_launcher_filter_re.match(m.name))

# Unfortunately the information about parallelization and privatization is
# lost by the current outliner, so rebuid it... :-(
kernel_launchers.privatize_module()
kernel_launchers.coarse_grain_parallelization()

# Add iteration space decoration and insert iteration clamping into the
# launchers onto the outer parallel loop nests:
kernel_launchers.gpu_loop_nest_annotate()

# Some verification output:
#kernel_launchers.display()

# End to generate the wrappers and kernel contents, but not the launchers
# that have already been generated:
kernel_launchers.gpu_ify(GPU_USE_LAUNCHER = False)

# Some verification output:
w.all_functions.display()
#w.compilation_units.display()

# Add communication around all the call sites of the kernels:
#kernel_launchers.kernel_load_store()
# Use a more specific communication location instead of the generic line above:
w["p4a_kernel_launcher_0"].kernel_load_store()
w["compute"].kernel_load_store()
kernel_launchers.display()

#kernel_launchers.gpu_loop_nest_annotate()
#kernel_launchers.display()

# Inline back the kernel into the wrapper, since CUDA can only deal with
# local functions if they are in the same file as the caller (by inlining
# them, by the way... :-) )
#kernel_filter_re = re.compile("p4a_kernel_\\d+$")
#kernels = w.filter(lambda m: kernel_filter_re.match(m.name))
# Should be no longer useful since the outliner generate now all the
# functions in the same compilation unit:
#kernels.inlining()

# Display the wrappers to see the work done:
kernel_wrapper_filter_re = re.compile("p4a_kernel_wrapper_\\d+$")
kernel_wrappers = w.filter(lambda m: kernel_wrapper_filter_re.match(m.name))
kernel_wrappers.display()

# Instead, do a global loop normalization above:
#kernels.loop_normalize()
#kernels.use_def_elimination()


#w.all.suppress_dead_code()
#w["main"].display()

# To be able to inject Par4All accelerator run time initializationlater:
w["main"].prepend_comment(PREPEND_COMMENT = "// Prepend here P4A_init_accel")

#w.all_functions.display(PRETTYPRINT_SEQUENTIAL_STYLE = "do")

# Unsplit resulting code, without OpenMP pragma since we want to generate
# GPU code :-) :
w.all.unsplit(PRETTYPRINT_SEQUENTIAL_STYLE = "do")

print("Generating final P4A code ready to be compiled:")
os.system("cd " + program + ".database; p4a_post_processor.py Src/*.c")
