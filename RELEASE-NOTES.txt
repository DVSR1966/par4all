Release notes for Par4All 1.0.5, 2010/12/22
===========================================

Ronan Keryell, HPC Project.

Par4All is an open-source environment to do source-to-source
transformations on C and Fortran programs for parallelizing, optimizing,
instrumenting, reverse-engineering, etc. on various targets, from embedded
multiprocessor system-on-chip with hardware accelerators up to high
performance computer and GPU.


Introduction
------------

This is the public version of Par4All. It parallelizes C programs to
OpenMP and CUDA, and Fortran programs to OpenMP.

More documentation and information is available on http://par4all.org.

For support you can mail to: support at par4all dot org

This release presents basic features of Par4All and PIPS. If you need more
complex transformations, parallelizations and optimizations, you may
contact HPC Project for professional support.

The main user interface in Par4All is the command-line interface ``p4a``
to invoke parallelization of the provided source codes, but also back-end
compilation or automatic ``CMakeFile`` generation.

But the other commands from the PIPS project included in Par4All are also
available: ``tpips`` and ``ipyps``. Of course, they are reserved for quite
more advanced users. For more information, look at http://pips4u.org

Since ``p4a`` is a script that interacts with PIPS to automatically
parallelize the source code in an average way, it is of course interesting
to dig into PIPS to apply specific transformations or change the value of
some parameters to get better performance on a given application.


Installation
------------

You can install Par4All in different ways, more or less automatic. The
installation is done into ``/usr/local/par4all``. If you want Par4All
installed in another location, use the compilation way of life at the end
of this section.

The best way if you are on GNU/Linux Debian or Ubuntu is to use
our package repository. This way, when a new version is out, your
classical package manager can automatically install it.

To use our package repository, pick *one* of the following lines, and
add it graphically with the *Update Manager* with
*Settings...*/*Third-Party Software* or append it with a text editor to
your ``/etc/apt/sources.list``, if you are using Ubuntu::

  deb http://download.par4all.org/apt/ubuntu releases main
  # --OR--
  deb http://download.par4all.org/apt/ubuntu development main

or if you are running Debian::

  deb http://download.par4all.org/apt/debian releases main
  # --OR--
  deb http://download.par4all.org/apt/debian development main

So you need to choose between ``releases`` or ``development``
versions. Development packages are generated often, may be unstable, and
are best suited if you want to track more closely the Par4All development.

Once this is done, run your favorite graphics package tool
(``synaptic``...)  or::

  sudo aptitude update
  sudo aptitude install par4all

And you are (almost) good to go! You will need to source a shell script as
described when you issue the ``aptitude install par4all`` command,
or log out and log in again for the environment to get properly set
up for Par4All. Once this is done, you can, for example, run ``p4a -h``
to get help about the usage of the ``p4a`` frontend script.

A less automatic way on Debian or Ubuntu is to install the Par4All
``.deb`` package you have found on http://download.par4all.org with::

  sudo gdebi <the_package>.deb

``sudo dpkg -i <the_package>.deb`` would also work but does not
automatically install dependencies you should install later.

An even less automatic way is to use a tar ball ``.tar.gz`` file. It
contains the binaries as built on a stable Ubuntu or unstable Debian
distribution. It should work on any GNU/Linux distribution with the
following libraries installed: (a fairly recent) ``libc.so.6``,
``libncurses.so.5``, ``libreadline.so.6``

Once you have downloaded one of these ``.tar.gz`` packages from
http://download.par4all.org, extract it with the following command::

  tar xvzf <the_package>.tar.gz

It will create a directory named ``par4all``. Move this directory to its final
location::

  sudo mv par4all /usr/local/

In any case, you will then need to source one of the following shell
scripts which set up the environment variables for proper Par4All
execution:

  - if you use ``bash``, ``sh``, ``dash``, etc.::

      source /usr/local/par4all/etc/par4all-rc.sh

  - if you use ``csh``, ``tcsh``, etc.::

      source /usr/local/par4all/etc/par4all-rc.csh

Then you can look at the manual of ``p4a`` on
http://www.par4all.org/documentation for information on how to use Par4All.

More details on the installation or how to compile Par4All from your own
can be found into the document describing all the infrastructure:
http://www.par4all.org/documentation/par4all-infrastructure


Examples:
---------

In the examples directory there are few examples showing some ``p4a``,
``tpips`` and ``pyps`` use cases.

Have a look at ``examples/README.txt``


Fortran
-------

The support for Fortran95 is not included in this release since it was not
mature enough at this release time, so only Fortran 77 with some
extensions is currently accepted.


C
-

Full C99 support is almost complete but right now there are nevertheless
few known limitationsÂ :

- variables in functions can only be defined at the beginning of a block,
  before any normal statement (like in C89)

- for a similar reason, loop index variable can not be declared directly
  in the loop such according the C99 syntax as in::

    {
      [...]
      for(int i = 0; i < N; i++) {
        [...]
      }
      [...]
    }

  but should be replaced with the C89 syntax as::

    {
      int i;
      [...]
      for(i = 0; i < N; i++) {
        [...]
      }
      [...]
    }

- you should avoid heavy use of pointers to enable a good
  parallelization. The point-to analysis is still on-going in PIPS

- there are still issues in the part that analyzes the control graph (the
  PIPS ``controlizer``). For example, right now we cannot deal well with a
  different hierarchy between the control graph and the variable
  scoping. For example if you have a block with a variable declaration
  within and a ``goto`` to/from outside, it may probably fail. This
  happens also if you have ``break`` or ``continue`` in loops with some local
  variables since those are internally represented with ``goto``.


Cuda
----

To compile and to run the CUDA output, you should have the following
environment variables set:

- ``CUDA_DIR`` to the directory where CUDA has been installed (default to
  ``/usr/local/cuda``)

- ``LD_LIBRARY_PATH`` should contains at least ``$CUDA_DIR/lib64``

Sometimes, Par4All generates C99 array parameter declarations in the
generated CUDA kernels that are not yet dealt with ``nvcc`` 3.2. To avoid this,
we apply an array to pointer transformation on generated kernels. We hope
nVidia will release soon a CUDA version that is C99-compliant as OpenCL
is. That means that in the mean time, the entry code should not contain
this kind of C99 function declaration::

  f_type my_function(size_t n, size_t m, float array[n][m]) {
     ...
  }

 instead you may have::

  f_type my_function(float array[n][m]) {
     ...
  }

with ``n`` and ``m`` that may be global variables, ``enum`` value or
``#define``.

There is another limitation in the nVidia CUDA 3.2 compiler that does not
accept function declarations with incomplete types such as::

  void saxpy (long long size, t_precision dst[], t_precision src[], t_precision alpha) {
    long long i = 0;
    for (i = 0; i < size; i++)
	  dst[i] = alpha * src[i] + dst[i];
  }

In this release the shared memory is not used yet, so to get the best
performance, loop nests need to be quite compute intensive, typically with
regular accesses to memory. Hopefully, the new cache architecture in Fermi
GPU is interesting to balance this limitation.

Par4All does not generate additional tiling yet, so right now the
iteration spaces of the 2 outer loops of a parallel loop nest are limited
by hardware GPU limits.


Embedded systems
----------------

With OpenMP output, as any other shared memory multiprocessor systems, all
the embedded systems that accept OpenMP can be addressed.

For example in the SCALOPES ARTEMIS European project, the code generation
for the Scaleo Chip Leon 3 MP-SoC is done through the OpenMP support of
the target.

Due to the immature state of the special code generation for the Ter@pix
SIMD image processor from Thales TRT (funded by FREIA ANR project) or the
SCMP dataflow architecture from CEA (funded by SCALOPES ARTEMIS European
project) are not included in this release.


SIMD and multimedia extensions
------------------------------

The SIMD (SSE) output is not included in this release yet since it was not
ready at release time but should be in a next release.


Python
------

On Debian, PLY seems to be installed only with Python 2.5. It works with
Python 2.6 and 3.1 with the ``PYTHONPATH`` generated for Par4All (with a
trailing ``:/usr/share/pyshared``). Unfortunately this breaks ``ipyps``.

On Ubuntu 10.04 there are also some configuration issues that make Python to
output harmless warning messages.

This text is typeset according to the reStructuredText Markup
Specification. For more information:
http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html

%%% Local Variables:
%%% mode: rst
%%% ispell-local-dictionary: "american"
%%% End:
