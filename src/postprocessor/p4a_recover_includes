#! /usr/bin/env python

# -*- coding: utf-8 -*-

"""
Recover #include into PIPS C output

The idea is to pass each standard C99 .h through PIPS and analyse the output and to store the association of a given C construction in the output to a give .h.

In this way, by parsing a PIPS output, we can statistically recover the
original #include <...h>

To use it, first
export PIPS_CPP=p4a_recover_includes -E
so that pips use this program to preprocess all the C PIPS input.

Initialize the database by analyzing all the standard .h with:
p4a_recover_includes --init
This is right now no longer used.


To recover #include in some PIPS output.c, use:
p4a_recover_includes output.c

How it works:

There are indeed 2 different methods that can be used:

- The simple method just analyzes the output of the preprocessor before
  feeding PIPS and adds a marker comments before and after a standard
  inclusion. This is done with --simple -E option.

  In simple cases, these markers are found again in the PIPS output and if
  we assume that the declarations are in the same order, we replace what
  is between 2 markers by the respective #include

  This is what is done by using then this program on the PIPS output with
  the --simple option

- The more complex method inserts a marker before every statement from a
  standard #include before feeding PIPS.

  After PIPS digestion, most of the marker comments are still attached to
  the declarations, but not for structures, typedef, unions... In complex
  cases (outlining, restructuring...) the code is reorganized to that
  unmarked elements that come from standard includes may be outside the
  markers from the simple method. So we need some learning to recognize
  the pieces that come from every standard include files. This should be
  done by the --init option once at the Par4All installation.

  Then, calling this program on the PIPS output without the --simple
  option replace each fragment from a standard include files with the
  respective #include at the place the first place a fragment appeared.

Ronan.Keryell@hpc-project.com
"""

#import string, re, sys, os, types, optparse

import re, optparse, pickle, os, shutil, subprocess, locale, p4a_util, shlex
import ply.lex as lex

#from pyps import *

# Debugging mode:
verbose = False
#verbose = True

# To select if we use a simple method:
simple_method = False

# The #include files we want to analyse:
# I use on the GNU libc'Appendix B Summary of Library Facilities' a
# grep '\.h' l | sed 's/.*`/    "/; s/'"'"'.*/",/' | sort -u
header_file_names = [
    "aio.h",
    "argp.h",
    "argz.h",
    "arpa/inet.h",
    "assert.h",
    "complex.h",
    "crypt.h",
    "ctype.h",
    "dirent.h",
    "envz.h",
    "err.h",
    "errno.h",
    "error.h",
    "execinfo.h",
    "fcntl.h",
    "fenv.h",
    "float.h",
    "fmtmsg.h",
    "fnmatch.h",
    "fstab.h",
    "ftw.h",
    "gconv.h",
    "getopt.h",
    "glob.h",
    "grp.h",
    "iconv.h",
    "inttypes.h",
    "langinfo.h",
    "libgen.h",
    "libintl.h",
    "limits.h",
    "locale.h",
    "malloc.h",
    "math.h",
    "mcheck.h",
    "mntent.h",
    "netdb.h",
    "net/if.h",
    "netinet/in.h",
    "nl_types.h",
    "obstack.h",
    "printf.h",
    "pthread.h",
    "pty.h",
    "pwd.h",
    "regex.h",
    "rpc/des_crypt.h",
    "sched.h",
    "search.h",
    "semaphore.h",
    "setjmp.h",
    "sgtty.h",
    "signal.h",
    "stdarg.h",
    "stddef.h",
    "stdio_ext.h",
    "stdio.h",
    "stdlib.h",
    "string.h",
    "sysctl.h",
    "sys/file.h",
    "sys/ioctl.h",
    "syslog.h",
    "sys/mman.h",
    "sys/mount.h",
    "sys/param.h",
    "sys/resource.h",
    "sys/socket.h",
    "sys/stat.h",
    "sys/sysinfo.h",
    "sys/time.h",
    "sys/times.h",
    "sys/timex.h",
    "sys/types.h",
    "sys/uio.h",
    "sys/un.h",
    "sys/utsname.h",
    "sys/vlimit.h",
    "sys/wait.h",
    "termios.h",
    "time.h",
    "ucontext.h",
    "ulimit.h",
    "unistd.h",
    "unistdh.h",
    "utmp.h",
    "utmpx.h",
    "varargs.h",
    "vtimes.h",
    "wchar.h",
    #"wcjar.h",
    "wctype.h",
    "wordexp.h",
    ]

#header_file_names = [ "stdio.h" ]

# End-of-file mark:
END_OF_INCLUDE_TO_FIND_MARKER = r'int p4a_recover_includes_end_of_parse;\n'

# The file name used for #include parsing:
tmp_header_file_name = "header.c"

# File name to use for persistance of information about headers and definitions
header_information_file_name = "header_information.pickle"


declaration_to_header = {}


def save_header_information():
    "Save the descriptions of the parsed structures in a file for later use"
    # Replace all the sets by frozensets so that we can use them as keys later
    for k, v in declaration_to_header.items():
        declaration_to_header[k] = frozenset(v)
    #print (declaration_to_header)

    # Create the file used to store the serialized version of
    # declaration_to_header:
    p = open(header_information_file_name, mode = 'bw')
    pickle.dump(declaration_to_header, p)
    p.close()


def load_header_information():
    "Load the descriptions of the parsed structures from the file generated by the previous header analysis"
    global declaration_to_header
    p = open(header_information_file_name, mode = 'br')
    declaration_to_header = pickle.load(p)
    p.close()
    #print (declaration_to_header)


def add_to_map_of_sets(d, key, value):
    """Associate to a key in a dictionnary d a set with a value
    d is a map of set of values
    """

    # Get the set associated to this key, if any:
    s = d.get(key)

    if not s:
        # Create a set for the key since there was nothing for this key:
        s = set()
        d[key] = s

    # Add the value to the set associated to the key:
    s.add(value)


class SimpleC99Lexer:
    """The PLY-lexer to parse C declarations with comments.
    The difficulty it to track nested { }"""

    # Declare the tokens:
    tokens = (
        'LBRACE',
        'RBRACE',
        'SEMICOLON_NL',
        'include',
        'comment',
        'string',
        'char'
        )


    # Match the '{':
    def t_LBRACE(self, t):
        r'\{'
        if verbose:
            print ("Found '{' at level", t.lexer.level)
        # Increase the nesting counter:
        t.lexer.level += 1
        return t


    # Match the '}':
    def t_RBRACE(self, t):
        r'\}'
        t.lexer.level -= 1
        if verbose:
            print ("Found '}' back to level", t.lexer.level)
        return t

    # Match the potential end of a declaration:
    t_SEMICOLON_NL = r';\n'

    # C99 comments, taken from pips/src/Libs/c_syntax/clex.l:
    t_comment = r'(/\*([^*]|(\*+[^/*]))*\*+/)|(//(\\\n|[^\n])*\n?)'


    # C99 string:
    t_string = r'"(\\"|[^"\\]|\\[^"])*"'

    # C99 character literal:
    t_char = r'\'([^\\\n]|(\\.))*?\''

    # A preprocessor position information about an included standard
    # include of the form '# 1 "/usr/include/features.h" 1 3 4'
    # See cpp-4.2 info '9 Preprocessor Output'

    #t_include = r'# 1 "/usr/include/[^"]+"[^\n]+\n'
    #t_include = r'\# .*1 [^\n]+\n'
    # For standard includes only:
    #t_include = r'\#[ ][1][ ]"/usr/include/[^"]+"[^\n]+\n'
    # For any include:
    t_include = r'\#[ ][1][ ]"[^"]+"[^\n]+\n'

    # For other characters, we store them as the current declaration:
    def t_error(self, t):
        if verbose:
            print("Found character '%s'" % t.value[0])
        # Keep the caracter as input
        t.lexer.content += t.value[0]
        # Remove it from the input:
        t.lexer.skip(1)

    # A string containing ignored characters (spaces and tabs)
    t_ignore  = ''

    # Build the lexer:
    def build(self, **kwargs):
        self.lexer = lex.lex(module=self, **kwargs)
        # To store the string of a declaration encountered:
        self.lexer.content = ""
        # Start without a nest { }:
        self.lexer.level = 0


class SimpleC99Lexer_simple:
    """The PLY-lexer to parse C declarations with comments.
    The difficulty it to track nested { }"""

    # Declare the tokens:
    tokens = (
        'LBRACE',
        'RBRACE',
        'SEMICOLON',
        'include',
        'comment',
        'string',
        'char'
        )


    # Match the '{':
    def t_LBRACE(self, t):
        r'\{'
        if verbose:
            print ("Found '{' at level", t.lexer.level)
        # Increase the nesting counter:
        t.lexer.level += 1
        return t


    # Match the '}':
    def t_RBRACE(self, t):
        r'\}'
        t.lexer.level -= 1
        if verbose:
            print ("Found '}' back to level", t.lexer.level)
        return t

    # C99 comments, taken from pips/src/Libs/c_syntax/clex.l:
    t_comment = r'(/\*([^*]|(\*+[^/*]))*\*+/\s*)|(//(\\\n|[^\n])*\n?)'


    # C99 string:
    t_string = r'"(\\"|[^"\\]|\\[^"])*"'

    # C99 character literal:
    t_char = r'\'([^\\\n]|(\\.))*?\''

    # A preprocessor position information about an included standard
    # include of the form '# 1 "/usr/include/features.h" 1 3 4'
    # See cpp-4.2 info '9 Preprocessor Output'

    #t_include = r'# 1 "/usr/include/[^"]+"[^\n]+\n'
    #t_include = r'\# .*1 [^\n]+\n'
    #t_include = r'#[^\n]+\n'
    t_include = r'\#[ ]\d+ [^\n]+\n'

    # Match the potential end of a declaration:
    t_SEMICOLON = r';'

    # For other characters, we store them as the current declaration:
    def t_error(self, t):
        if verbose:
            print("Found character '%s'" % t.value[0])
        # Keep the caracter as input
        t.lexer.content += t.value[0]
        # Remove it from the input:
        t.lexer.skip(1)

    # A string containing ignored characters (spaces and tabs)
    t_ignore  = ''

    # Build the lexer:
    def build(self, **kwargs):
        self.lexer = lex.lex(module=self, **kwargs)
        # To store the string of a declaration encountered:
        self.lexer.content = ""
        # Start without a nest { }:
        self.lexer.level = 0


def parse_declarations(header_name, file_name):
    "Keep track of all the declarations generated by this header file digested by PIPS"
    f = open(file_name)
    # First skip the PIPS-generated header that is 3 line long:
    for i in range(3):
        f.readline()
    # slurp all the file in a string:
    content = f.read()
    f.close()

    # Hum, the following is rather an invariant and should be moved out of
    # the loop...
    # Build the lexer:
    l = SimpleC99Lexer()
    l.build()
    # Give the lexer some input:
    l.lexer.input(content)

    # Tokenize:
    for tok in l.lexer:
        l.lexer.content += tok.value
        if verbose:
            print(tok)
        if tok.type == 'SEMICOLON_NL' and l.lexer.level == 0:
            # A semicolon ending a line at {}-nesting level 0
            # is a complete declaration:
            if verbose:
                print (tok.value, "Found declaration:\n", l.lexer.content)
            # Note that this declaration is to be found in this header_name:
            add_to_map_of_sets(declaration_to_header,
                               l.lexer.content,
                               header_name)
            # Reset the content variable:
            l.lexer.content = ""
    # For later use of the lexer, if any, throw away useless characters:
    l.lexer.content = ""


def init_headers():
    "Read classical header files to construct index tables"

    # A directory to store PIPS-processed include files:
    try:
        os.mkdir('headers')
    except:
        pass

    errors = open('failing_headers', mode = 'w')

    for n in header_file_names:
        # Create a .c file that includes this header file:
        f = open(tmp_header_file_name, mode = 'w')
        #with python 2.x
        print >>f, '#include <' + n + '>'
        #this is python 3.1
        #print ('#include <' + n + '>', file = f)
        #print('int p4a_#include <' + n + '>', file = f)
        f.close()

        if False:
            #  No longer used

            # Create a proxy include file to add a marking comment that is
            # used to recognize more precizely the included file because
            # of the massive cross inclusions of C99 .h...
            pn = os.path.join('include', n)
            # Crate the directory for it:
            try:
                os.mkdir(os.path.dirname(pn))
            except:
                # Just in case it already exists...
                pass
            # And then the file:
            
            proxy = open(pn, mode = 'w')
            #python 2.X
            print >>proxy, '/* This file includes ' + n + ' for PIPS */'
            print >>proxy, '#include "' + os.path.join('/usr/include', n ) + '"'
            
            #python3.1
            #print('/* This file includes ' + n + ' for PIPS */',
            #      file = proxy)
            #print('#include "' + os.path.join('/usr/include', n ) + '"',
            #      file = proxy)
            proxy.close()

        #w = workspace(tmp_header_file_name);
        #print w
        #parse_declarations(n, 'a.database/header!/header!.pre.c')
        #os.system("tpips header.tpips; mkdir -p headers include")
        os.system("tpips header.tpips")
        try:
            parse_declarations(n, 'header.database/Src/header.c')
            # Keep a copy of the header digested by PIPS if manual
            # inspection is needed:
            shutil.copy('header.database/Src/header.c',
                        os.path.join('headers', n))
        except IOError:
            # Notice the .h that can not get through PIPS:
            #python2.x
            print >>errors, 'The parsing by PIPS seems to have failed on'
            #python3.1
            #print('The parsing by PIPS seems to have failed on', n,
            #     file = errors)


def sort_key(declaration):
    "Not clear to Ronan Keryell how this sort_key concept works yet"
    return len(declaration)


#match_a_line = re.compile(r'^(.*)$', re.MULTILINE)


def recover_header(file_name, output_file_name):
    """Replace CPP-inlined stuff from previous #include <something.h> by
    the #include <something.h> itself"""

    # To store the set of found headers:
    found_headers = set()
    # Since an output can be generated by different #include because of
    # some .h including some other one, keep track of all the sets of .h
    # that could be responsible of an entry:
    found_headers_sets = set()
    f = open(file_name)
    # slurp all the file in a string:
    content = f.read()
    f.close()
    #print(declaration_to_header)

    # Build a list of list of declarations to find sorted by decreasing
    # size to be sure that the longer matches will be done first:
    declarations = list(declaration_to_header.keys())
    declarations = sorted(declarations)
    #print(declarations)
    #for i in declarations:
    #    print(i)
    #declarations = sorted(declarations, sort_key, True)
    #declarations = sorted(declarations, (lambda x: len(x)), True)

    for d in declarations:
        h = declaration_to_header[d]
        #print(h, d)
        if content.find(d) != -1:
            # Add the header name to the found names
            found_headers |= h
            found_headers_sets.add(h)
            if verbose:
                print(h)
                print(d)
            # Replace the header generated content:
            # Since we will use it as a regex later, protect all the special
            # characters in it before using them:
            # This could have been done at save/pickle time for
            # efficiency, but we stress debugability here...
            hgc = re.escape(d);
            (content, n_substitutions) = re.subn(hgc, '', content)
            if verbose and n_substitutions > 0:
                print('Found', n_substitutions, 'substitutions for:')
                #print(hgc)
            #print(content)
            #exit(0)
    print('Found headers:', found_headers)
    print('All the sets of headers found:', found_headers_sets)

    #print(content)
    if not output_file_name:
        # If no output file is provided, override the input file:
        output_file_name = file_name

    # Write the result:
    f = open(output_file_name, mode = 'w')
    # Output the unchanged 3-line header of the changed file:
    for i in range(3):
        # Get in line the first line of the content and get the remaining
        # in content:
        (line, content) = re.split(r'\n', content, 1)
        # Output the line to the destination file:
        print >>f, line 

    # Then write the recoverd includes:
    for h in found_headers:
		#python2.X
		print >>f, '#include <', h, '>',
		#print3.1
        #print('#include <', h, '>', sep = '', file = f)

    # Finished with the remaining of the file:
    print >>f, content
    f.close()


def postpreprocess(output_string):
    "Add a comment on all the begin of standard include files"
    # Build the lexer:
    l = SimpleC99Lexer()
    l.build()
    # Give the lexer some input:
    l.lexer.input(output_string)

    # Tokenize:
    for tok in l.lexer:
        if verbose:
            print(tok)
        if tok.type == 'include':
            # Do not forget any pending other characters:
            #python2.x
            print l.lexer.content,
            #pyhton 3.1
            #print(l.lexer.content, end = '')
            l.lexer.content = ""
            # Add a comment marking the begin of standard inclusion. Strip
            # the trailing \n of the include sentence.
            print('/* Par4All including', tok.value[:-1], '*/')

        # Display with any pending other characters before:
        l.lexer.content += tok.value
        #python2.x
        print l.lexer.content,
        #python3.1
        #print(l.lexer.content, end = '')

        l.lexer.content = ""

    # Do not forget other pending characters if any:
    #python2.x
    print l.lexer.content,
    #python3.1
    #print(l.lexer.content, end = '')
    # For later use of the lexer, if any, throw away useless characters:
    l.lexer.content = ""


# To parse the # pragma from gcc/cpp:
parse_line_pragma_re = re.compile(r'\# (\d+) ([<"][^">]+[">]) (\d) ?(\d)? ?(\d)?\n')

include_stack = []

def postpreprocess_simple(output_string):
    """Add a comment on all the begin of standard include files before
    PIPS digestion so that we can detect their inclusion later and recover
    them in recover_header_simple() in the PIPS output"""

    # Build the lexer:
    l = SimpleC99Lexer_simple()
    l.build()
    # Give the lexer some input:
    l.lexer.input(output_string)

    def flush_lexer_output(content):
        """Print l.lexer.content and content
        and reset l.lexer.content
       """
        #python2.x
        print l.lexer.content + content,
        #python3.1
        #print(l.lexer.content + content, end = '')
        l.lexer.content = ""


    # Tokenize:
    for tok in l.lexer:
        if verbose:
            print(tok, include_stack)
        #print(tok, include_stack)
        if tok.type == 'include':
            #print(tok.value, end = '')
            m = parse_line_pragma_re.match(tok.value)
            if m:
                file_name = m.group(2)
                #print(file_name,m.group(3),m.group(4),m.group(5))

                # A preprocessor position information about an included
                # standard include of the form '# 1
                # "/usr/include/features.h" 1 3 4' See cpp-4.2 info '9
                # Preprocessor Output'

                # To normalize standard include names:
                #python2.6
                file_name = re.sub(r'^"/usr/include/(.*)"$',
                                   r'<\1>', file_name, 0)
                #python2.7
                #file_name = re.sub(r'^"/usr/include/(.*)"$',
                #                   r'<\1>', file_name, 0, 0)

                if m.group(3) == "1":
                    include_stack.append(file_name)
                    if verbose:
                        print('Including', file_name, len(include_stack))
                    if simple_method and len(include_stack) == 1:
                        # In the case of the simple method, just mark
                        # begin of #include that are at the top-level:
                        flush_lexer_output('/* P4A include '
                                           + file_name + ' starts here */\nvoid p4a_dummy_recover();\n')
                    #print(include_stack)
                if m.group(3) == "2":
                    returning_from = include_stack.pop()
                    if verbose:
                        print('Returning from',
                              returning_from,
                              len(include_stack))
                    if simple_method and len(include_stack) == 0:
                        # In the case of the simple method, just mark
                        # the end of #include that are at the top-level:
                        flush_lexer_output('/* P4A include '
                                           + returning_from
                                           + ' stops here */void p4a_dummy_recover();\n')
        elif include_stack and not simple_method:
            # We are in an inclusion, so decorate only full statements:
            if tok.type == 'comment':
                # Remove them since the comments are now include markers
                # we generate:
                pass
            else:
                # Keep the content
                l.lexer.content += tok.value
                if tok.type == 'SEMICOLON' and l.lexer.level == 0:
                    # If not using the simple method, we mark each
                    # statement:
                    # The end of a construction, output a comment,
                    # followed by the construction
                    print('\n/* P4A include tracking:',
                          include_stack[0], '*/')
                    flush_lexer_output("")
        else:
            # No special processing:
            # Keep the content and print it:
            flush_lexer_output(tok.value)

    # Flush the remaining output if any:
    flush_lexer_output("")


def sanitize_recover_output(content):
    """Replace in the content string some work overdone by the
    gcc preprocessor on Linux.

    Not really portable and do some global rewriting without subtlety..."""

    # Sanitize stuff like
    # '#include "/usr/lib/gcc/x86_64-linux-gnu/4.4.3/include/stdbool.h"'
    # or
    # '#include "/usr/lib/gcc/x86_64-linux-gnu/4.4.3/include-fixed/limits.h"'
    # into '#include <stdbool.h>'
    content = re.sub(r'#include "/usr/lib/gcc/[^/]+/[^/]+/include(-fixed)?/([^"]+)"',
                     '#include <\\2>', content)
    # Recover C99 floating point constants:
    content = re.sub(r'\(__huge_val\.__d\)',
                     'HUGE_VAL', content)
    content = re.sub(r'\(__huge_valf.__f\)',
                     'HUGE_VALF', content)
    content = re.sub(r'\(\(long double\) HUGE_VAL\)',
                     'HUGE_VALL', content)
    # TODO to be completed...

    return content


def recover_header_simple(file_name, output_file_name):
    """Replace CPP-inlined stuff from previous #include <something.h> by
    the #include <something.h> itself using a simple method"""

    f = open(file_name)
    # slurp all the file in a string:
    content = f.read()
    f.close()

    # Create the output file:

    # By default, override the input file:
    if not output_file_name:
        output_file_name = file_name
    output = open(output_file_name, mode='w')

    # The string output:
    so = ""

    def flush_lexer_output(content):
        """Append to output l.lexer.content and content and reset
        l.lexer.content
       """
        so += l.lexer.content + content
        l.lexer.content = ""


    if simple_method:
        #content = re.sub(r'/\* P4A include "/usr/include/([^"]+)" starts here \*/.+?/\* P4A include "/usr/include/\1" stops here \*/',
        #        r'#include <\1>', content, 0, re.DOTALL)
        #python2.6
        content = re.sub(r'/\* P4A include ([<"][^">]+[">]) starts here \*/(.|\n)+?/\* P4A include \1 stops here \*/',
                r'#include \1', content, 0)
        #python2.7
        #content = re.sub(r'/\* P4A include ([<"][^">]+[">]) starts here \*/.+?/\* P4A include \1 stops here \*/',
        #       r'#include \1', content, 0, re.DOTALL)
        # Remove dummy declaration, was there to help PIPS to hold comments
        content = re.sub(r'void p4a_dummy_recover\(\);\n',
                r'', content, 0)

        so += content
        if verbose:
            print('on ', output_file_name)

    else:
        # Else, the complex method...

        # To store the set of found headers:
        found_headers = set()

        # Build the lexer:
        l = SimpleC99Lexer_simple()
        l.build()

        # Give the lexer some input:
        l.lexer.input(content)

        # The current header encountered, used also as a FSM:
        header = None

        # Tokenize:
        for tok in l.lexer:
            if verbose:
                print(tok)
            if tok.type == 'comment':
                #print(tok.value, end = '')
                m = parse_pseudo_comment_re.match(tok.value)
                if m:
                    header = m.group(1)
                    if verbose:
                        print('Header used:', header)
                    found_headers.add(header)
                else:
                    # Normal comment
                    flush_lexer_output(tok.value)
            elif header:
                # We are skipping up to the end of the statement since it will
                # be replaced later by a #include:
                if tok.type == 'SEMICOLON' and l.lexer.level == 0:
                    if verbose:
                        print('End of header used:', header)
                    # It is the end of the statement, revert in normal mode:
                    header = None
                    # Throw away the pending text:
                    l.lexer.content = ""
                else:
                    # Just wait for the future end of statement:
                    # Just in case we are in a weird file:
                    l.lexer.content = ""
            else:
                # Normal content & output:
                l.lexer.content += tok.value
                #python2.x
                print l.lexer.content,
                #python3.1
                #print(l.lexer.content, end = '')
                # Reset the content buffer:
                l.lexer.content = ""
        # Flush the output
        #python2.x
        print l.lexer.content,
        #python3.1
        #print(l.lexer.content, end = '')
        # For later use of the lexer, if any, throw away useless characters:
        l.lexer.content = ""

    # Output the result to the file:
    #python2.x
    print >>output, sanitize_recover_output(so),
    #python3.1
    #print(sanitize_recover_output(so), end = '', file = output)


def postpreprocess(output_string):
    "Add a comment on all the begin of standard include files"
    # Build the lexer:
    l = SimpleC99Lexer_simple()
    l.build()
    # Give the lexer some input:
    l.lexer.input(output_string)

    # Tokenize:
    for tok in l.lexer:
        if verbose:
            print(tok)
        if tok.type == 'include':
            #print(tok.value, end = '')
            m = parse_line_pragma_re.match(tok.value)
            if m:
                file_name = m.group(2)
                #print(file_name,m.group(3),m.group(4),m.group(5))
                if m.group(3) == "1":
                    include_stack.append(file_name)
                    if verbose:
                        print('Including', file_name, len(include_stack))
                    #print(include_stack)
                if m.group(3) == "2":
                    if verbose:
                        print('Returning from', include_stack.pop(), len(include_stack))
                    pass
        elif include_stack:
            # We are in an inclusion, so decorate only full statements:
            if tok.type == 'comment':
                # Remove comments
                pass
            else:
                # Keep the content
                l.lexer.content += tok.value
                if tok.type == 'SEMICOLON' and l.lexer.level == 0:
                    # The end of a construction, output a comment,
                    # followed by the construction
                    print('/* P4A include tracking:', include_stack[0], '*/')
                    # Remove leading \n:
                    content = re.sub('^\s+', '', l.lexer.content)
                    print(content)
                    l.lexer.content = ""
        else:
            # No special processing:
            # Keep the content and print it:
            l.lexer.content += tok.value
            #print(l.lexer.content, end = '')
            print(l.lexer.content)
            # Reset the content buffer:
            l.lexer.content = ""

    # Flush the output
    #python2.x
    print l.lexer.content,
    #python3.1
    #print(l.lexer.content, end = '')
    # For later use of the lexer, if any, throw away useless characters:
    l.lexer.content = ""


def preprocess(args):
    """Call the preprocessor with the given argument and postprocess a
    little bit the output to add comments to better track standard file
    inclusion.

    This is an optparse-callback to be able to get trailing arguments"""
                
    # Pick the default values from
    # pips/src/Libs/preprocessor/preprocessor-local.h:
    cpp_args = shlex.split(" -C  -D__PIPS__ -D__HPFC__ -U__GNUC__ ")
    
    # Add other options from the PIPS_CPP_FLAGS environment variable if defined:
    if 'PIPS_CPP_FLAGS' in os.environ:
        cpp_args += shlex.split(os.environ['PIPS_CPP_FLAGS'] + " ")
    
    # Addded the trailing arguments of this program as options or files to
    # preprocess, if any:
    cpp_args += args

    # Mark all the arguments as having been consumed:
    #python2.x
    output_string = subprocess.Popen(['cpp']+cpp_args, stdout=subprocess.PIPE).communicate()[0]

    #python3.1
    #output_string = subprocess.check_output("cpp" + cpp_args, shell=True)
    if verbose:
        print('The output before', output_string)
    # Well, add a postprocess phase in the... preprocessor phase :-)
    # Transform the byte array into string before
    #postpreprocess(output_string.decode())

    #python3.1
    # In Python 3 everything is internally in UTF-8, so decode the string
    # first:
    # encoding = locale.getdefaultlocale()[1]
    # postpreprocess_simple(output_string.decode(encoding))
    #python2.x
    # In Python 2.x encoding concept is still fuzzy...
    postpreprocess_simple(output_string)


def record_preprocess_options(option, opt_str, value, parser):
    """Notice to call the preprocessor on the given argument and
    postprocess a little bit the output to add comments to better track
    standard file inclusion.

    This is an optparse-callback to be able to get trailing arguments"""

    #print (option, opt_str, value, parser, parser.rargs)

    # Add all the remaining parser arguments into
    # parser.options. Duplicate parser.rargs since we delete its elements
    # the line after and they are lost then:
    setattr(parser.values, "preprocessor_args", list(parser.rargs))
    # Mark all the arguments as having been consumed:
    del parser.rargs[:]


def main():
    global verbose, simple_method

    parser = optparse.OptionParser(usage = "usage: %prog [options] [<files>]",
                                   version = "$Id")

    parser.add_option("-i",  "--init",
                     action = "store_true", dest = "init", default = False,
                     help = "Initialize from PIPS exersizing the #include tables that will be used in the postprocessor")

    parser.add_option("-E",  "--preprocessor",
                     action = "callback", callback = record_preprocess_options,
                      help = "Use this program as a preprocessor. It follows the PIPS_CPP_FLAGS environment variable but not PIPS_CPP since often the later is used to call this program. It must be the last option of this program before all the other options and parameters to be passed to CPP. This option override the -i/--init and -o/--output options. The arguments passed to the CPP are the PIPS default options, the content of the PIPS_CPP_FLAGS environment variable and the trailing arguments of this programm.")

    parser.add_option("-s",  "--simple",
                      action = "store_true", dest="simple", default = False,
                      help = "Use a simple method that should work for simple case and that avoid previous learning of the standard include files")

    parser.add_option("-o",  "--output", dest="output_file_name",
                      help = "When used in default postprocessor mode, it sets the name of the file used to output the recovered includes instead of overriding the input file",
                      metavar="FILE")

    group = optparse.OptionGroup(parser, "Debug options")

    group.add_option("-v",  "--verbose",
                     action = "store_true", dest = "verbose", default = False,
                     help = "Run in verbose mode")

    group.add_option("-q",  "--quiet",
                     action = "store_false", dest = "verbose",
                     help = "Run in quiet mode [default]")

    parser.add_option_group(group)

    (options, args) = parser.parse_args()

    verbose = options.verbose
    simple_method = options.simple

    if options.init:
        # Asked for initialize the include recovering database:
        init_headers()
        save_header_information()
    elif getattr(options, "preprocessor_args", None):
        # Use this tool as a specific preprocessor:
        preprocess(options.preprocessor_args)
    else:
        if args:
            # If there are still args, this program is used as a #include
            # recovery tool.
            # Then first load the mapping information
            ###load_header_information()
            # An then recover #include in each file:
            for file_name in args:
                recover_header_simple(file_name, options.output_file_name)


# If this programm is independent it is executed:
if __name__ == "__main__":
    main()

# Some Emacs stuff:
### Local Variables:
### mode: python
### mode: flyspell
### ispell-local-dictionary: "american"
### End:
