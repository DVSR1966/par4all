#! /usr/bin/env python3.1

# -*- coding: utf-8 -*-

"""
Recover #include into PIPS C output

The idea is to pass each standard C99 .h through PIPS and analyse the output and to store the association of a given C construction in the output to a give .h.

In this way, by parsing a PIPS output, we can statistically recover the
original #include <...h>

To use it, first
export PIPS_CPP=p4a_recover_includes -E
so that pips use this program to preprocess all the C PIPS input.

Initialize the database by analyzing all the standard .h with:
p4a_recover_includes --init

To recover #include in some PIPS output.c, use:
p4a_recover_includes output.c

Ronan.Keryell@hpc-project.com
"""

#import string, re, sys, os, types, optparse

import re, optparse, pickle, os, shutil, subprocess
import ply.lex as lex

#from pyps import *

# The #include files we want to analyse:
# I use on the GNU libc'Appendix B Summary of Library Facilities' a
# grep '\.h' l | sed 's/.*`/    "/; s/'"'"'.*/",/' | sort -u
header_file_names = [
    "aio.h",
    "argp.h",
    "argz.h",
    "arpa/inet.h",
    "assert.h",
    "complex.h",
    "crypt.h",
    "ctype.h",
    "dirent.h",
    "envz.h",
    "err.h",
    "errno.h",
    "error.h",
    "execinfo.h",
    "fcntl.h",
    "fenv.h",
    "float.h",
    "fmtmsg.h",
    "fnmatch.h",
    "fstab.h",
    "ftw.h",
    "gconv.h",
    "getopt.h",
    "glob.h",
    "grp.h",
    "iconv.h",
    "inttypes.h",
    "langinfo.h",
    "libgen.h",
    "libintl.h",
    "limits.h",
    "locale.h",
    "malloc.h",
    "math.h",
    "mcheck.h",
    "mntent.h",
    "netdb.h",
    "net/if.h",
    "netinet/in.h",
    "nl_types.h",
    "obstack.h",
    "printf.h",
    "pthread.h",
    "pty.h",
    "pwd.h",
    "regex.h",
    "rpc/des_crypt.h",
    "sched.h",
    "search.h",
    "semaphore.h",
    "setjmp.h",
    "sgtty.h",
    "signal.h",
    "stdarg.h",
    "stddef.h",
    "stdio_ext.h",
    "stdio.h",
    "stdlib.h",
    "string.h",
    "sysctl.h",
    "sys/file.h",
    "sys/ioctl.h",
    "syslog.h",
    "sys/mman.h",
    "sys/mount.h",
    "sys/param.h",
    "sys/resource.h",
    "sys/socket.h",
    "sys/stat.h",
    "sys/sysinfo.h",
    "sys/time.h",
    "sys/times.h",
    "sys/timex.h",
    "sys/types.h",
    "sys/uio.h",
    "sys/un.h",
    "sys/utsname.h",
    "sys/vlimit.h",
    "sys/wait.h",
    "termios.h",
    "time.h",
    "ucontext.h",
    "ulimit.h",
    "unistd.h",
    "unistdh.h",
    "utmp.h",
    "utmpx.h",
    "varargs.h",
    "vtimes.h",
    "wchar.h",
    #"wcjar.h",
    "wctype.h",
    "wordexp.h",
    ]

#header_file_names = [ "stdio.h" ]

# End-of-file mark:
END_OF_INCLUDE_TO_FIND_MARKER = r'int p4a_recover_includes_end_of_parse;\n'

# The file name used for #include parsing:
tmp_header_file_name = "header.c"

# File name to use for persistance of information about headers and definitions
header_information_file_name = "header_information.pickle"


verbose = False
declaration_to_header = {}


def save_header_information():
    "Save the descriptions of the parsed structures in a file for later use"
    # Replace all the sets by frozensets so that we can use them as keys later
    for k, v in declaration_to_header.items():
        declaration_to_header[k] = frozenset(v)
    #print (declaration_to_header)

    # Create the file used to store the serialized version of
    # declaration_to_header:
    p = open(header_information_file_name, mode = 'bw')
    pickle.dump(declaration_to_header, p)
    p.close()


def load_header_information():
    "Load the descriptions of the parsed structures from the file generated by the previous header analysis"
    global declaration_to_header
    p = open(header_information_file_name, mode = 'br')
    declaration_to_header = pickle.load(p)
    p.close()
    #print (declaration_to_header)


def add_to_map_of_sets(d, key, value):
    """Associate to a key in a dictionnary d a set with a value
    d is a map of set of values
    """

    # Get the set associated to this key, if any:
    s = d.get(key)

    if not s:
        # Create a set for the key since there was nothing for this key:
        s = set()
        d[key] = s

    # Add the value to the set associated to the key:
    s.add(value)


class SimpleC99Lexer:
    """The PLY-lexer to parse C declarations with comments.
    The difficulty it to track nested { }"""

    # Declare the tokens:
    tokens = (
        'LBRACE',
        'RBRACE',
        'SEMICOLON_NL',
        'std_include',
        'comment',
        'string',
        'char'
        )


    # Match the '{':
    def t_LBRACE(self, t):
        r'\{'
        if verbose:
            print ("Found '{' at level", t.lexer.level)
        # Increase the nesting counter:
        t.lexer.level += 1
        return t


    # Match the '}':
    def t_RBRACE(self, t):
        r'\}'
        t.lexer.level -= 1
        if verbose:
            print ("Found '}' back to level", t.lexer.level)
        return t

    # Match the potential end of a declaration:
    t_SEMICOLON_NL = r';\n'

    # C99 comments, taken from pips/src/Libs/c_syntax/clex.l:
    t_comment = r'(/\*([^*]|(\*+[^/*]))*\*+/)|(//(\\\n|[^\n])*\n?)'


    # C99 string:
    t_string = r'"(\\"|[^"\\]|\\[^"])*"'

    # C99 character literal:
    t_char = r'\'([^\\\n]|(\\.))*?\''

    # A preprocessor postion information about an included standard
    # include of the form '# 1 "/usr/include/features.h" 1 3 4'
    # See cpp-4.2 info '9 Preprocessor Output'

    #t_std_include = r'# 1 "/usr/include/[^"]+"[^\n]+\n'
    #t_std_include = r'\# .*1 [^\n]+\n'
    t_std_include = r'\#[ ][1][ ]"/usr/include/[^"]+"[^\n]+\n'

    # For other characters, we store them as the current declaration:
    def t_error(self, t):
        if verbose:
            print("Found character '%s'" % t.value[0])
        # Keep the caracter as input
        t.lexer.content += t.value[0]
        # Remove is from the input:
        t.lexer.skip(1)

    # A string containing ignored characters (spaces and tabs)
    t_ignore  = ''

    # Build the lexer:
    def build(self, **kwargs):
        self.lexer = lex.lex(module=self, **kwargs)
        # To store the string of a declaration encountered:
        self.lexer.content = ""
        # Start without a nest { }:
        self.lexer.level = 0


def parse_declarations(header_name, file_name):
    "Keep track of all the declarations generated by this header file digested by PIPS"
    f = open(file_name)
    # First skip the PIPS-generated header that is 3 line long:
    for i in range(3):
        f.readline()
    # slurp all the file in a string:
    content = f.read()
    f.close()

    # Hum, the following is rather an invariant and should be moved out of
    # the loop...
    # Build the lexer:
    l = SimpleC99Lexer()
    l.build()
    # Give the lexer some input:
    l.lexer.input(content)

    # Tokenize:
    for tok in l.lexer:
        l.lexer.content += tok.value
        if verbose:
            print(tok)
        if tok.type == 'SEMICOLON_NL' and l.lexer.level == 0:
            # A semicolon ending a line at {}-nesting level 0
            # is a complete declaration:
            if verbose:
                print (tok.value, "Found declaration:\n", l.lexer.content)
            # Note that this declaration is to be found in this header_name:
            add_to_map_of_sets(declaration_to_header,
                               l.lexer.content,
                               header_name)
            # Reset the content variable:
            l.lexer.content = ""
    # For later use of the lexer, if any, throw away useless characters:
    l.lexer.content = ""


def init_headers():
    "Read classical header files to construct index tables"

    # A directory to store PIPS-processed include files:
    try:
        os.mkdir('headers')
    except:
        pass

    errors = open('failing_headers', mode = 'w')

    for n in header_file_names:
        # Create a .c file that includes this header file:
        f = open(tmp_header_file_name, mode = 'w')
        print('#include <' + n + '>', file = f)
        #print('int p4a_#include <' + n + '>', file = f)
        f.close()

        if False:
            #  No longer used

            # Create a proxy include file to add a marking comment that is
            # used to recognize more precizely the included file because
            # of the massive cross inclusions of C99 .h...
            pn = os.path.join('include', n)
            # Crate the directory for it:
            try:
                os.mkdir(os.path.dirname(pn))
            except:
                # Just in case it already exists...
                pass
            # And then the file:
            proxy = open(pn, mode = 'w')
            print('/* This file includes ' + n + ' for PIPS */',
                  file = proxy)
            print('#include "' + os.path.join('/usr/include', n ) + '"',
                  file = proxy)
            proxy.close()

        #w = workspace(tmp_header_file_name);
        #print w
        #parse_declarations(n, 'a.database/header!/header!.pre.c')
        #os.system("tpips header.tpips; mkdir -p headers include")
        os.system("tpips header.tpips")
        try:
            parse_declarations(n, 'header.database/Src/header.c')
            # Keep a copy of the header digested by PIPS if manual
            # inspection is needed:
            shutil.copy('header.database/Src/header.c',
                        os.path.join('headers', n))
        except IOError:
            # Notice the .h that can not get through PIPS:
            print('The parsing by PIPS seems to have failed on', n,
                  file = errors)


def sort_key(declaration):
    "Not clear to Ronan Keryell how this sort_key concept works yet"
    return len(declaration)


#match_a_line = re.compile(r'^(.*)$', re.MULTILINE)


def recover_header(file_name, output_file_name):
    """Replace CPP-inlined stuff from previous #include <something.h> by
    the #include <something.h> itself"""

    # To store the set of found headers:
    found_headers = set()
    # Since an output can be generated by different #include because of
    # some .h including some other one, keep track of all the sets of .h
    # that could be responsible of an entry:
    found_headers_sets = set()
    f = open(file_name)
    # slurp all the file in a string:
    content = f.read()
    f.close()
    #print(declaration_to_header)

    # Build a list of list of declarations to find sorted by decreasing
    # size to be sure that the longer matches will be done first:
    declarations = list(declaration_to_header.keys())
    declarations = sorted(declarations)
    #print(declarations)
    #for i in declarations:
    #    print(i)
    #declarations = sorted(declarations, sort_key, True)
    #declarations = sorted(declarations, (lambda x: len(x)), True)

    for d in declarations:
        h = declaration_to_header[d]
        #print(h, d)
        if content.find(d) != -1:
            # Add the header name to the found names
            found_headers |= h
            found_headers_sets.add(h)
            if verbose:
                print(h)
                print(d)
            # Replace the header generated content:
            # Since we will use it as a regex later, protect all the special
            # characters in it before using them:
            # This could have been done at save/pickle time for
            # efficiency, but we stress debugability here...
            hgc = re.escape(d);
            (content, n_substitutions) = re.subn(hgc, '', content)
            if verbose and n_substitutions > 0:
                print('Found', n_substitutions, 'substitutions for:')
                #print(hgc)
            #print(content)
            #exit(0)
    print('Found headers:', found_headers)
    print('All the sets of headers found:', found_headers_sets)

    #print(content)
    if not output_file_name:
        # If no output file is provided, override the input file:
        output_file_name = file_name

    # Write the result:
    f = open(output_file_name, mode = 'w')
    # Output the unchanged 3-line header of the changed file:
    for i in range(3):
        # Get in line the first line of the content and get the remaining
        # in content:
        (line, content) = re.split(r'\n', content, 1)
        # Output the line to the destination file:
        print(line, file = f)

    # Then write the recoverd includes:
    for h in found_headers:
        print('#include <', h, '>', sep = '', file = f)

    # Finished with the remaining of the file:
    print(content, file = f)
    f.close()


def postpreprocess(output_string):
    "Add a comment on all the begin of standard include files"
    # Build the lexer:
    l = SimpleC99Lexer()
    l.build()
    # Give the lexer some input:
    l.lexer.input(output_string)

    # Tokenize:
    for tok in l.lexer:
        if verbose:
            print(tok)
        if tok.type == 'std_include':
            # Do not forget any pending other characters:
            print(l.lexer.content, end = '')
            l.lexer.content = ""
            # Add a comment marking the begin of standard inclusion. Strip
            # the trailing \n of the include sentence.
            print('/* Par4All including', tok.value[:-1], '*/')

        # Display with any pending other characters before:
        l.lexer.content += tok.value
        print(l.lexer.content, end = '')

        l.lexer.content = ""

    # Do not forget other pending characters if any:
    print(l.lexer.content, end = '')
    # For later use of the lexer, if any, throw away useless characters:
    l.lexer.content = ""


def preprocess(option, opt_str, value, parser):
    """Call the preprocessor on the given argument and postprocess a little bit the output to add comments to better track standard file inclusion.

    This is an optparse-callback to be able to get trailing arguments"""

    #print (option, opt_str, value, parser, parser.rargs)

    # Pick the default values from
    # pips/src/Libs/preprocessor/preprocessor-local.h:
    cpp_args = " -C  -D__PIPS__ -D__HPFC__ -U__GNUC__ "
    # Add other options from the PIPS_CPP_FLAGS environment variable if defined:
    if 'PIPS_CPP_FLAGS' in os.environ:
        cpp_args += os.environ['PIPS_CPP_FLAGS'] + " "
    # Addded the trailing arguments of this program as files to
    # preprocess, if any:
    cpp_args += " ".join(parser.rargs)
    # Mark all the arguments as having been consumed:
    del parser.rargs[:]
    # Rely on the shell to easily use the CPP parameters:
    output_string = subprocess.check_output("cpp" + cpp_args, shell=True)
    if verbose:
        print('The output before', output_string)
    # Well, add a postprocess phase in the... preprocessor phase :-)
    # Transform the byte array into string before
    postpreprocess(output_string.decode())


def main():
    global verbose

    parser = optparse.OptionParser(usage = "usage: %prog [options] [<files>]",
                                   version = "$Id")

    parser.add_option("-i",  "--init",
                     action = "store_true", dest = "init", default = False,
                     help = "Initialize from PIPS exersizing the #include tables that will be used in the postprocessor")

    parser.add_option("-E",  "--preprocessor",
                     action = "callback", callback = preprocess,
                      help = "Use this program as a preprocessor. It follows the PIPS_CPP_FLAGS environment variable but not PIPS_CPP since often the later is used to call this program. It must be the last option of this program before all the other options and parameters to be passed to CPP. This option override the -i/--init and -o/--output options. The arguments passed to the CPP are the PIPS default options, the content of the PIPS_CPP_FLAGS environment variable and the trailing arguments of this programm.")

    parser.add_option("-o",  "--output", dest="output_file_name",
                      help = "When used in default postprocessor mode, it sets the name of the file used to output the recovered includes instead of overriding the input file",
                      metavar="FILE")

    group = optparse.OptionGroup(parser, "Debug options")

    group.add_option("-v",  "--verbose",
                     action = "store_true", dest = "verbose", default = False,
                     help = "Run in verbose mode")

    group.add_option("-q",  "--quiet",
                     action = "store_false", dest = "verbose",
                     help = "Run in quiet mode [default]")

    parser.add_option_group(group)

    (options, args) = parser.parse_args()

    verbose = options.verbose

    if options.init:
        init_headers()
        save_header_information()
    else:
        if args:
            # If there are still args, this program is used as a #include
            # recovery tool.
            # Then first load the mapping information
            load_header_information()
            # An then recover #include in each file:
            for file_name in args:
                recover_header(file_name, options.output_file_name)


# If this programm is independent it is executed:
if __name__ == "__main__":
    main()
