\batchmode
\documentstyle[12pt]{article}
\input{/usr/share/local/lib/tex/macroslocales/Dimensions.tex}
\newcommand{\titre}{RAPPORT D'AVANCEMENT No 1 \\
                    ANALYSE SYNTAXIQUE ET ANALYSE SEMANTIQUE
}
\newcommand{\auteur}{Franc,ois IRIGOIN \\
                     Pierre JOUVELOT \\
                     Re'mi TRIOLET
}
\newcommand{\docdate}{De'cembre 1988}
\newcommand{\numero}{E104}

\begin{document}
\sloppy
\input{/usr/share/local/lib/tex/macroslocales/PageTitre.tex}

Nous pre'sentons dans ce rapport l'e'tat d'avancement du projet PIPS en
ce qui concerne:
\begin{itemize}
\item   la reception du mate'riel,
\item   l'analyseur lexical,
\item   la de'finition de la repre'sentation interne,
\item   l'analyseur syntaxique,
\item   l'analyseur se'mantique.
\end{itemize}

\section{Mate'riel}

\paragraph{}
Nous avons a` ce jour rec,u la quasi totalite' du mate'riel
commande' pour l'exe'cution de ce contrat. Le serveur est
ope'rationel dans sa configuration comple`te depuis le de'but du mois
de Septembre, quand aux stations clientes, nous les avons rec,ues
une par une, entre de'but Octobre et fin Novembre. Les stations
clientes ne sont pas comple`tes, puisqu'elles n'ont e'te' livre'es
qu'avec 4 Mo de me'moire au lieu des 8 commande's. Sun France nous
promet la me'moire manquante pour Mars 1989.

\paragraph{}
L'installation des ces quatre machines s'est faite sans gros proble`me,
car nous avons pu profiter de l'expe'rience d'autres chercheurs de
l'Ecole des Mines de Paris qui se sont e'quippe's avant nous. Nos
quatre machines composent la majeure partie du re'seau du CAI, qui
comportera d'ci la fin de l'anne'e 7 stations: 1 serveur Sun, 5
stations clientes Sun, et un serveur Hewlett-Packard. 

\paragraph{}
L'Ecole des Mines de Paris est d'autre part en train de de'velopper un
{\em re'seau inter centres (RIC)} pour relier ses diffe'rents centres de
recherche. Le re'seau du CAI sera relie' au RIC de`s que possible,
sans doute a` la fin du permier trimestre 1989.

\section{Analyse lexicale}

L'analyse lexicale de Fortran pose quelques proble`mes puisque ce
langage ne contient pas de mots clefs re'serve's comme c'est le cas de
langages plus re'cents tels que Pascal ou C.

Par exemple, on de'tecte que l'instruction suivante est une affectation
car le caracte`re qui suit la parenthe`se fermant la re'fe'rence au
tableau IF est le caracte`re '='.
\begin{verbatim}
IF(I, J, K) = 6.66
\end{verbatim}

\paragraph{}
En conse'quence, l'utilitaire {\em lex}, disponible sous UNIX, ne
permet pas de re'aliser un analyseur lexical pour Fortran. Une
premie`re solution consistait donc a` e'crire comple`tement un
analyseur lexical pour Fortran, ce qui aurait repre'sente' beaucoup de
travail.

Nous avons pre'fe're' de'composer l'analyse lexicale de Fortran en
deux parties, une premie`re partie ayant pour objet de lever les
ambiguite's contenues dans un programme Fortran gr\^ace a` une
pre'-analyse qui introduit des mots clefs au de'but de chaque
instruction, et une seconde partie, beaucoup plus simple car base'e sur
lex, qui re'alise l'analyse syntaxique du Fortran avec mots clefs produit
par la premie`re partie.

\subsection{Pre'-analyseur lexical}

La premie`re partie revient a` fournir a` l'utilitaire {\em yacc} une
fonction 'getc' qui permette de lever les difficulte's lie'es a`
Fortran.

La nouvelle fonction getc fonctionne de la fac,on suivante.  Getc lit
d'un seul coup toutes les lignes d'une instruction Fortran, c'est a`
dire la ligne initiale et les 19 e'ventuelles lignes de continuation, et
les stocke dans le buffer 'Stmt'.  Au vol, getc repe`re le label,
enle`ve tous les blancs, de'tecte les caracte`res entre simples
quotes, et met a` jour 4 variables externes, qui repre'sentent pour
l'instruction courante la premie`re et la dernie`re ligne commentaire,
et la premie`re et la dernie`re ligne source.  Ensuite, le contenu du
buffer Stmt est analyse' pour y de'tecter les mot clefs, c'est a`
dire traiter les cas des instructions IF, ELSEIF, ASSIGN, DO, des
de'claratives IMPLICIT et FUNCTION, et des operateurs {\em .XX.} (.EQ.,
.NEQV., ...).

Lorsqu'un mot clef est de'tecte', il est mis en miniscules dans le
texte source, sauf la premie`re lettre qui reste en majuscule.  Ainsi,
lex peut faire la diffe'rence entre le mot clef 'Assign' et
l'identificateur 'ASSIGN'.  Gr\^ace a` la premie`re lettre, lex peut
de'tecter deux mots clef successifs, me^me sans blanc pour les
se'parer, comme dans 'IntegerFunctionASSIGN(X)'.

Lorsqu'un ope'rateur .XX. est de'tecte', il est remplace' dans le
source par \verb+_XX_+.  Ainsi, lex peut faire la difference entre une
constante re'elle et un ope'rateur, comme dans \verb/(X+1._EQ_5)/.

\subsection{Post-analyseur lexical}

La seconde partie est tout a` fait classique, c'est une spe'cification
d'analyseur lexical, dans le langage propose' par lex. Cette
spe'cification se compose d'une liste d'expressions re'gulie`res
correspondant aux tokens du langage, ave{c}, pour chacune d'entre elles,
le code du token a` renvoyer a` yacc lorsqu'un token de ce type est
de'tecte' dans le programme source.

\paragraph{}
Nous donnons en annexe le texte source de l'analyseur lexical de PIPS.

\section{Repre'sentation interne}

La repre'sentation interne (RI) de PIPS n'est pas encore parfaitement
de'finie. Nous avons plusieurs objectifs que nous nous efforc,ons de
respecter pour le design de la RI:
\begin{itemize}
\item utilisation de Newgen;
\item inde'pendance vis a` vis de Fortran;
\item simplicite'.
\end{itemize}

Voici quelques explications sur ces trois objectifs.

\subsection{L'outil Newgen}

Newgen\footnote{{\em Newgen: a Language-Independant Program Generator},
Pierre Jouvelot et Re'mi Triolet, Rapport Interne ENSMP-CAI en cours de
pre'paration} est un outil de spe'cification logiciel de'veloppe' au CAI
dans le but de faciliter la conception et l'imple'mentation de
programmes manipulant des structures de donne'es complexes. A partir de
la de'finition en langage de haut niveau de structures de donne'es,
Newgen ge'ne`re automatiquement l'ensemble des fonctions ne'cessaires a`
la manipulation de telles structures (cre'ation, acce`s et
modification). De plus, il est possible de sauvegarder et de relire sur
fichier des structures de donne'es ge're'es par Newgen.  Ces fonctions
(ou macros) peuvent e^tre e'crites dans tout langage de programmation
supporte' par Newgen.  Les langages actuellement traite's sont C et
CommonLISP.

A titre d'exemple, voici la manie`re dont pourrait e^tre de'crite une
structure de donne'e repre'sentant un arbre syntaxique:
\begin{verbatim}
expression = constante:int + unaire:expression + binaire
binaire = gauche:expression x droite:expression x operateur:string
\end{verbatim}
A partir d'une telle de'finition, Newgen ge'ne`re automatiquement des
fonctions comme {\tt make\_expression} pour cre'er des expressions, {\tt
expression\_constante\_p} pour tester l'appartenance d'une expression a`
la sous-classe des expressions constantes, {\tt binaire\_gauche} pour
acce'der au fils gauche d'une expression binaire, etc.  Il est e'galement
possible d'e'crire (via {\tt gen\_write}) et lire (via {\tt gen\_read}) des
structures ge're'es par Newgen. Parmi les autres caracte'ristiques de
Newgen, signalons la possibilite' de libe'rer la place alloue'e a` des
objets ({\tt gen\_free}), de de'clarer des types complexes comme des
listes ou tableaux multi-dimensionnels, de tester a` l'exe'cution la
conformite' d'une structure avec sa de'claration Newgen, de tabuler des
objets, de ge'rer l'e'criture et la lecture de donne'es circulaires ou
partage'es,...

Nous avons de'cide' d'utiliser Newgen dans le cadre du projet Pips pour
plusieurs raisons :
\begin{itemize}
\item
        Newgen permet de de'finir les structures de donne'es manipule'es
par Pips dans un langage de haut-niveau, fournissant de fait une
documentation automatique de Pips et imposant une certaine normalisation
dans l'e'criture des logiciels par l'utilisation des fonctions
automatiquement cre'e'es par Newgen,
\item
        Newgen est utile dans une phase de prototypage car il permet de
tester diffe'rents choix de structures de donne'es sans ne'cessiter de
recodage majeur, ce qui est un avantage dans un projet de recherche
comme Pips,
\item
        Newgen permet de tester la cohe'rence des structures de donne'es
cre'e'es par Pips, soit en ``real-tim{e}'' par un syste`me de typage
ge're' automatiquement par Newgen, soit au moment des e'critures et
relectures de structures de donne'es sur fichier,
\item
        Newgen permet la compatibilite' au niveau fichier entre les
parties de Pips e'crites dans des langages diffe'rents (ainsi, par
exemple, l'analyseur syntaxique base' sur Yacc est e'crit en C et le
pretty-printer l'est en CommonLISP), permettant ainsi l'e'volution
``douc{e}'' de modules de CommonLISP (en phase de prototypage) vers C si
des questions de performance le requie`rent.  A noter que cette
compatibilite' peut e^tre encore plus inte'ressante si on utilise les
``pipes'' d'Unix pour e'viter les acce`s-disque.
\item
        Newgen autorise l'utilisation de types de donne'es
pre'-existants ce qui permet la re'utilisation de logiciels e'crits en
dehors de l'environnement Newgen.
\end{itemize}

\subsection{Inde'pendance vis a` vis de Fortran}

Il nous a paru important pour le projet PIPS d'e^tre le plus possible
inde'pendant du langage Fortran, me^me si Fortran reste le langage
favori des nume'riciens. La plupart des constructeurs vont d'ailleurs
dans ce sens puisqu'on voit de plus en plus apparaitre des
compilateurs-vectoriseur capables aussi bien de vectoriser Fortran, C ou
Ada.

L'expe'rience que nous avons jusqu'a` pre'sent montre que le surplus de
travail ne'cessaire pour s'abstraire de Fortran est tre`s faible, et
qu'au contraire, l'analyse pousse'e de Fortran que nous avons due faire
nous a permis de mieux comprendre les diverses constructions de ce langage.

\subsection{Simplicite' de la RI}

La simplicite' de la RI permet de limiter la quantite' de code a`
e'crire pour re'aliser les phases d'analyses et de transformations de
programme qui s'appuieront sur la RI.

Notre objectif principal a e'te', encore une fois, de s'abstraire de
Fortran le plus possible pour permettre a` un plus grand nombre de
constructions syntaxiques d'utiliser les me^mes structures de donne'es.
Ceci permet de limiter le volume de code a` e'crire puisqu'a` chaque
structure de donne'es devra correspondre des fonctions d'analyse et de
transformation. Donnons quelques exemples d'abstractions.

Les variables scalaires n'existent pas dans notre RI car elles sont
repre'sente'es par des tableaux a` 0 dimension. Les constantes
n'existent pas non plus car elles sont repre'sente'es par des appels a`
des fonctions sans code, dont la valeur est la valeur de la constante.
Le me^me principe a e'te' adopte' pour les ope'rateurs.
Il en re'sulte que les structures de donne'es pour stoker les
expressions sont tre`s simples:
\begin{verbatim}
expression = reference + call

reference = variable:entity x indices:expression*

call = function:entity x args:expression*
\end{verbatim}

Ces trois lignes signifient que:
\begin{itemize}
\item   une expression est un appel de fonction (\verb+call+) ou une
re'fe'rence a` une variable (\verb+reference+);

\item   une re'fe'rence est compose'e d'une entre'e dans la table
des symboles (\verb+entity+) et d'une liste d'expressions (\verb+expression*+) qui sont
les indices;

\item   un call est compose'e d'une entre'e dans la table des symboles
(\verb+entity+) et d'une liste d'expressions (\verb+expression*+) qui sont les
arguments de l'appel;
\end{itemize}

Nous n'avons que quatre types d'instructions dans notre RI: le test, la
boucle, le goto et le call. La plupart des instructions Fortran (STOP,
RETURN, AFFECTATION, READ, WRITE, ...) sont transforme'es en des calls
a` des fonctions intrinsics, dont le nom permet de retrouver, si besoin
est, l'instruction Fortran d'origine.

\paragraph{}

Un rapport complet sur la RI sera fourni avec le prochain rapport sur
l'analyse syntaxique.

\section{Analyse syntaxique}

L'analyse syntaxique de PIPS sera re'alise'e avec l'utilitaire yacc. Cet
utilitaire permet de de'finir une grammaire par un ensemble de re`gles
construites sur les tokens du langage et sur d'autres symboles appele's
symboles non terminaux. Voici quelques exemples de re`gles.
\begin{verbatim}
linstruction: instruction TK_EOS
        | linstruction instruction TK_EOS
        ;

expression: reference
        | call
        | constante
        | signe expression
        | expression TK_PLUS expression
        | expression TK_MINUS expression
        | ...
        ;
\end{verbatim}

La premie`re re`gle signifie qu'une liste d'instructions est compose'e
de plusieurs instructions se'pare'es par des tokens \verb+TK_EOS+ (token
end-of-statement). La seconde re`gle signifie qu'une expression est soit
une re'fe'rence, soit un call, soit une constante, soit un signe suivie
d'une expression, soit la somme ou la diffe'rence de deux expressions.
Les symboles call, constante, reference, ... sont des non-terminaux, et
doivent donc e^tre de'finis plus loin en fonction des tokens du langage.

\paragraph{}
Yacc permet en plus d'associer a` chaque re`gle une portion de code
e'crit en langage C, qui est exe'cute'e chaque fois que la re`gle en
question est reconnue dans le programme Fortran soumis a` l'analyseur.
L'exemple suivant montre une partie de la re`gle instruction, et la
portion de code associe'e; il s'agit dans ce cas d'un appel de fonction
pour chainer l'instruction que l'on vient de reconnaitre au bloc
d'instructions courant.
\begin{verbatim}
instruction: return_inst
            { LinkInstToCurrentBlock($1); }
        | ...
        ;
\end{verbatim}

\paragraph{}
L'analyse syntaxique est en cours de de'veloppement, et le
texte source de cette partie sera fournie avec le prochain rapport.

\section{Analyse se'mantique}

Un des objectifs du projet PIPS est d'e'tudier l'inte're^t d'une
analyse se'mantique approfondie pour la paralle'lisation
interproce'durale. La me'thode choisie, de'veloppe'e par P. Cousot
et N. Halbwachs, fournit des e'galite's et ine'galite's line'aires
entre variables scalaires entie`res. Ces e'galite's et ine'galite's
ge'ne'ralisent les techniques habituelles en optimisation globale:
propagation de constante, de'tection de variables inductives,
de'tection d'e'galite's line'aires entre variables, indices
appartenant a` l'intervalle de'fini par les bornes de boucles. Ces
e'galite's et ine'galite's line'aires ont de plus l'avantage de
pouvoir e^tre aise'ment utilise'es dans le calcul du graphe de
de'pendance. Rappelons que cette analyse de'taille'e n'est
effectue'e qu'au niveau intra-proce'dural et qu'une technique plus
simple est pre'vue au niveau interproce'dural.

Ce premier rapport pre'sente dans une premie`re partie l'architecture
ge'ne'rale de l'analyseur se'mantique et dans une deuxie`me partie la
premie`re couche de modules mathe'matiques, un package de vecteur
creux.

\subsection{Architecture ge'ne'rale de l'analyseur se'mantique}

L'analyseur se'mantique consiste a` associer aux e'tats me'moires des
interpre'tations non-standard, ge'ne'ralement des approximations, et
aux instructions des ope'rateurs sur ces interpre'tations. Des
ope'rateurs supple'mentaires sont introduits pour prendre en compte
les noeuds de jonction du graphe de contro^le, qu'ils servent a` fermer
un test ou une boucle.

Il est donc possible de distinguer deux phases inde'pendantes. La
premie`re est la traduction du graphe de contro^le (control flow
graph) de'core' par des instructions en un syste`me d'e'quations aux
polye`dres (un syste`me d'e'galite's et d'ine'galite's peut e^tre
vu comme un polye`dre). (un syste`me d'e'galite's et
d'ine'galite's peut e^tre vu comme un polye`dre).

La deuxie`me phase consiste a` re'soudre le syste`me ou plus
exactement a` en trouver un point fixe aussi bon que possible. Il ne
reste plus alors qu'a` rattacher les re'sultats aux noeuds graphes de
contro^le.

\subsubsection{Traduction du graphe de contro^le} 

La traduction du graphe de contro^le se de'compose a` nouveau en deux
activite's distinctes. Chaque instruction doit d'abord e^tre traduite
en une ope'ration sur des polye`dres. Les affectations affines,
inversibles ou non, et les tests line'aires se traduisent bien mais il
peut aussi arriver qu'aucune information ne soit disponible et que
certaines instructions se traduisent en une simple perte d'information.
Cette traduction de'pend malheureusement elle-me^me des connaissances
se'mantiques que nous avons du programme. Par exemple l'instruction
\verb+I=J*K+ n'est pas a priori une transformation line'aire. Mais elle le
devient si on de'tecte que K est une constante valant par exemple 4.

Pour e'viter ce proble`me la traduction des instructions est
retarde'e dans l'analyseur SYNTOX et reporte'e a` la phase de
re'solution. Nous n'avons pas retenu cette solution pour le moment
parce que la traduction d'une instruction en une suite d'ope'ration sur
les polye`dres est cou^teuse en temps CPU et parce qu'elle devrait
e^tre effectue'e a` chaque ite'ration de la re'solution du syste`me.
Cela n'est pas aussi ge'nant dans SYNTOX. Il manipule des intervalles
de'finis inde'pendemment sur chaque variable. Il est aise' de
re'e'crire les ope'rateurs usuels des langages de programmation en
ope'rateurs sur les intervalles.

Cela veut donc dire qu'il faudrait faire pre'ce'der l'analyse
se'mantique "a` la Cousot" par une propagation interproce'durale de
constantes. Ces constantes permettraient de trouver davantage
d'ope'rations line'aires et re'duiraient la dimensionalite' des
polye`dres a` traiter (leur dimension est e'gale a priori aux nombres de
variables scalaires entie`res du module analyse').

Une autre solution consisterait a` proce'der a` plusieurs analyses
se'mantiques successives et a` utiliser les re'sultats de l'analyse
pre'ce'dente pour effectuer la traduction.

La deuxie`me activite' consiste a` associer les bons ope'rateurs aux
noeuds de jointure et a` se'lectionner les points auxquels il faudra
appliquer des e'largissements (en analyse avant). L'ope'ration
d'e'largissement permet de trouver un point fixe en un nombre limite'
d'ite'rations au prix d'une perte d'information. Cette perte
d'information conduit a` de mauvais re'sultats si ces points sont mal
choisis et si aucune strate'gie de convergence locale dans les boucles
les plus internes n'est de'finie.

\subsubsection{Re'solution du syste`me aux polye`dres} 

Cette phase de'bute avec en entre'e un syste`me d'e'quations aux polye`dres et
une suite de groupe de noms de polye`dres. Une solution triviale base'e sur le
botton du treillis des polynomes et sur les e'le'ments neutres d'ope'rateurs
est fixe'e. Elle est ensuite propage'e ite'rativement a` travers le syste`me
d'e'quations jusqu'a` convergence, en ne prenant en compte que les polye`dres
du 2e`me groupe et ainsi de suite jusqu'a` ce qu'on ait trouve' un point fixe
global, valable pour tous les polye`dres.

Les premiers polye`dres traite's sont les invariants des boucles les plus
internes puis on remonte ensuite vers le top level en ajoutant les
invariants des bandes interme'diaires.

Cette re'solution ite'rative est tre`s co^uteuse car les ope'rations
e'le'mentaires constituent chacune un proble`me line'aire complexe
mettant en jeu les deux repre'sentations des polye`dres, les syste`mes
d'e'galite's et d'ine'galite's et les syste`mes ge'ne'rateurs, et
pre'sentant une complexite' exponentielle. Un gros effort va e^tre fait
pour optimiser ces modules, dont certains serviront d'ailleurs aussi au
test de de'pendance et a` la ge'ne'ration de code, et pour profiter du
caracte`re creux des vecteurs manipule's.

\subsubsection{Phase terminale de l'analyse} 

Il faut calculer les invariants associe's a` chaque point du graphe de
contro^le a` partir des invariants calcule's ite'rativement uniquement
pour les premiers noeuds de chaque intervalle ou bloc de base du graphe
de contro^le.

Il faut ensuite essayer de factoriser ces invariants sur les graphes de
contro^le structure's pour e'viter de re'pe'ter sur chaque noeud une
information comme UNIT=5 et pour diminuer l'espace me'moire utilise'.

Finalement les invariants doivent e^tre attache's directement aux noeuds du
graphe de contro^le pour en permettre l'utilisation par les phases
ulte'rieures de PIPS.

\subsection{Manipulation de vecteurs creux} 

Les vecteurs constituent la premie`re couche de structures de donne'es
conduisant aux syste`mes line'aires d'e'galite's et d'ine'galite's et
aux syste`mes ge'ne'rateurs.

Expe'rimentalement, ces vecteurs dont la longueur est e'gale au nombre de
constantes scalaires entie`res sont tre`s creux car seule une ou deux
variables et une constante y apparaissent. Les coefficients des autres
variables sont nuls et n'ont pas besoin d'e^tre repre'sente's.

Tous les algorithmes sont base's sur des re'pe'titons de combinaisons
line'aires de tels vecteurs. Elles doivent donc s'effectuer tre`s
vite.

D'autres packages (e'galite's, ine'galite's, matrices, syste`mes
d'ine'galite's) seront base'es sur ce premier package de vecteur
creux.

\end{document}
\end
