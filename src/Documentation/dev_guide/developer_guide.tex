%%
%% $Id$
%%
%
% PIPS development environment documentation;
% translated and updated from dret146.f.tex

\documentclass[a4paper]{article}
\usepackage[latin9]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[backref,pagebackref]{hyperref}
\makeatletter
% Test for a TeX4ht command:
%%\@ifundefined{HCode}{%
%%  % We are generating PDF, so allow to break down URL:
%%  \usepackage{breakurl}}{%
%%  % else we are generating HTML with TeX4ht, use plain url:
%%  \usepackage{url}}
\usepackage{url}
\usepackage{xspace,makeidx}
\usepackage[hyper,procnames]{listings}

\sloppy

%% heu...
\lstset{extendedchars=true, language=C, basicstyle=\footnotesize\ttfamily, numbers=left,
  numberstyle=\tiny, stepnumber=2, numberfirstline=true, showspaces=true,
  showstringspaces=false, showtabs=true,
  tabsize=8, tab=\rightarrowfill, keywordstyle=\bf,
  stringstyle=\rmfamily, commentstyle=\rmfamily\itshape,
  index=[1][keywords],indexprocnames=true}

\newcommand{\LINK}[1]{\url{#1}\xspace}

\newcommand{\PipsOldWWW}{\LINK{http://www.cri.ensmp.fr/pips/}\xspace}
\newcommand{\PipsNewWWW}{\LINK{https://freia.enstb.org/pips}\xspace}
\newcommand{\PipsDevGuidePDF}{\LINK{http://www.cri.ensmp.fr/pips/developer_guide.htdoc/developer_guide.pdf}}
\newcommand{\PipsDevGuideHTDOC}{\LINK{http://www.cri.ensmp.fr/pips/developer_guide.htdoc}}
\newcommand{\DoxygenSources}{\LINK{https://freia.enstb.org/pips/source}}

\title{\bf {\Huge PIPS}\\
  Development Environment\\
}

\author{
\begin{tabular}{rl}
  François & IRIGOIN\\
  Pierre & JOUVELOT\\
  Rémi & TRIOLET\\
  Arnauld & LESERVOT\\
  Alexis & PLATONOFF\\
  Ronan & KERYELL\\
  Béatrice & CREUSILLET\\
  Fabien & COELHO\\
  Corinne & ANCOURT
\end{tabular}
}

\date{August 1996 - June 2008}

\renewcommand{\indexname}{Index}
\makeindex

% Number everything in the TOC:
\setcounter{secnumdepth}{10}
\setcounter{tocdepth}{10}

\begin{document}
\maketitle

{\small\vfill\hfill
 \verb$Id$
}

You can get a printable version of this document on\\
\PipsDevGuidePDF and a HTML version on \PipsDevGuideHTDOC.

The legacy WWW site is at \PipsOldWWW and a new one is at \PipsNewWWW.

\clearpage
\tableofcontents

\newpage

\section{Introduction}

This document aims at presenting PIPS development environment. It is
not linearly organized: PIPS is made of several, sometimes
interdependent, components.  This paper thus begins with a
presentation of PIPS directories.  Then the shell environment is
described. The next two chapters are devoted to two external tools on
which PIPS relies: NewGen and the Linear library.
Section~\ref{sec:makefiles} will then present PIPS make file policy.
Sections~\ref{sec:library_internal_organization}
and~\ref{sec:pass_organization} are devoted to PIPS libraries and
passes. The next section briefly describes some conventions usually
respected when developing in PIPS. The last two sections describe
the \emph{bug policy} of PIPS and some save and restore information.

This manual is not exhaustive. You can add your own sections and update
existing ones if you find missing or erroneous information.

The reader is supposed to be a PIPS user~\cite{Pips:96a}, and to have read
the reports about NewGen~\cite{Jouv:89,Jouv:90}. A good understanding of
\texttt{pipsmake} mechanisms would also be helpful.

\section{Getting PIPS sources}

The sources of NewGen, Linear and Pips are managed under subversion.  They
are accessible from anywhere in the world by the http protocol at
\url{http://svn.cri.ensmp.fr/pips.html}

\subsection{Directories overview}
\label{sec:directories-overview}

There are 5 repositories for the various files:
\begin{description}
\item[nlpmake] common makefiles for Newgen, Linear and Pips.
\item[newgen] Newgen software.
\item[linear] Linear/C3 mathematical libraries.
\item[pips] PIPS software.
\item[validation] Pips non-regression tests
\end{description}

The subversion repositories are organised in standard subdirectories as
advised for best subversion practices:
\begin{description}
\item[trunk] production version, should be stable enough
  to pass the validation;
\item[branches] development branches of all or part of
  the software for each developer. Developments are to be performed
  here and installed (joined, merged) once finished into \texttt{trunk};
\item[tags] tagged revisions. Note these tags are not related with the
  tags described in section~\ref{subsec:tags}.
\end{description}

Moreover the \texttt{pips} repository includes
\begin{description}
\item[bundles:] group of softwares;
\item[bundles/trunks:] convenient extraction at once of all the 3
  softwares trunk versions ready for compilation.
\end{description}

\subsection{Building PIPS}
\label{sec:building-pips}


A crude script \texttt{setup\_pips.sh} on the web site and in
\texttt{pips/trunk/makes} allows to download \texttt{polylib},
\texttt{newgen}, \texttt{linear} and \texttt{pips} and to build a local
installation of the softwares.  For instance developer \texttt{calvin} can
do the following to setup its own PIPS environment:
\begin{verbatim}
sh> setup_pips.sh /home/temp/MYPIPS calvin
...
sh> source /home/temp/MYPIPS/pipsrc.sh
sh> # enjoy
\end{verbatim}

In order to build only the PIPS infrastructure, just do \texttt{make} into
the \verb|$PIPS_ROOT| directory.

For more information about the PIPS Makefile infrastructure, see

\subsection{Important Note}

It is important to realize that:
\begin{enumerate}
\item PIPS build is much more efficient if it is stored on local disks rather
  than remote directories accesses through the network (for instance via NFS).
  Hence the \texttt{/home/temp} directory choice in the above axample.
\item the local copy editions are not saved unless commits are performed.
\item in a standard development, commits should not be made directly under
  the \texttt{trunk}, but rather in development branches, see the next section.
\item on some Unises, the \texttt{/tmp} temporary directories may be cleaned
  on reboots, which can be triggered by power failures. So you could try
  \texttt{/var/tmp} that is usually not cleaned-up during the boot phase.
\end{enumerate}


\section{Developing environment}
\label{sec:devel-envir}


\subsection{Browsing the source files with Doxygen}
\label{sec:brows-source-files}

To help digging into the sources, Doxygen is used to generate an
interactive on-line version of the sources that can be seen at
\DoxygenSources. The call and caller graphs are useful to figure out what
are the functions used, and more subtly what are the functions that call a
given function (if you need to verify a function is correctly used or you
want to change a function name).

As an experimented user, you may need to generate them again. Go to the
\texttt{doxygen} directories in the documentation directory of the
products (NewGen, Linear and PIPS) and use \texttt{make} and \texttt{make
  publish} to push them on the WWW site. The \texttt{make} target
\texttt{doxygen-plain} or \texttt{doxygen-graph} can be used to generate
only a documentation without or with call and caller graphs. Note that
generating the graph version for PIPS lasts several hours...


\subsection{Developing with the Emacs editor}
\label{sec:developing-with-emacs}

There are many modes in Emacs that can help developments in PIPS.

To compile the sources you can use the menu \texttt{Tools/Compile...}. It
presents errors in red and if you click on them you jump to the sources at
the right location.

The check-compilation-errors-on-the-fly is quite fun (it is equivalent in
programming to the on-the-fly spell checker \texttt{flyspell-mode}). PIPS
makefiles support it. Try it with \texttt{M-x flymake-mode}.

The \texttt{speedbar} mode helps with a navigation window. More descrete,
to have access from the menu to functions and other symbols in the local
source file, test the \texttt{imenu-add-to-menubar} Emacs function.

You can use tags to index source files as explained in
section~\ref{sec:using-tags}.

Most PIPS contributors use Emacs to develop in PIPS, so some goodies have
been added to ease those developers.

Have a look for example at section~\ref{sec:newgen} for debugging. You
should use the \texttt{gud-tooltip-modes} to display the value of an
object under the mouse. Setting the \texttt{gdb-many-windows} to
\texttt{t} transform Emacs (from version 22) in an interesting graphical
IDE. You can select a frame by just clicking it and so on.

Most documentation (like the one you are reading) is written in LaTeX and
some parts even use literate programming with a \LaTeX{} base, so the
AUCTeX, PreviewLaTeX and RefTeX mode are quite useful.


\subsection{Using tags to index source files and to ease access in your
  text editor}
\label{sec:using-tags}

Many macros and functions are available from the Linear and NewGen
libraries, but also in PIPS. Their source code can be retrieved using the
\emph{tags} indexing mechanism under editors such as \textbf{emacs} or
\textbf{vi}.

So the PIPS makefiles can generate tags index files to find the definition
of symbols in the source files with your favorite editor and it is highly
recommended to use the Emacs or VI editors to benefit from the tags.  For
example in Emacs, you can find, by typing a \texttt{M-.} on an identifier,
the location where it is defined, \texttt{M-,} allows to find another
location. Note that completion with \texttt{TAB} works with tag searching
too. With \textbf{vi}, you can use \texttt{C-]} to jump on a definition of
an identifier.


PIPS can produce tags files for VI by invoking \verb|make CTAGS| or tags
for Emacs by invoking \verb|make TAGS| in the PIPS \texttt{prod} top
directory and it will produce the tags files into the \texttt{linear},
\texttt{newgen} and \texttt{pips} subdirectories.

To build the tags for both VI and Emacs, just use a simple \texttt{make
  tags}.  The tags files at the PIPS top directories need to be remade
regularly to have an up to date version of the index.  should be used for
\textbf{vi} users.

You can also build specifically the tags for a development version of PIPS
for example by making these tags explicitly in the \texttt{newgen/trunk},
\texttt{pips/trunk} and \texttt{linear/trunk} directories.

Then you need to configure your editor to use these tags files, for
example in Emacs by using the customization of the \texttt{Tags Table
  List} with the \texttt{Options/Customize Emacs} menu or more
directly in you \texttt{.emacs} with stuff like:
\begin{itemize}
\item for a production version of the tags :
\begin{verbatim}
(custom-set-variables
  ;; ...
 '(tags-table-list (quote
    ("/home/keryell/projets/PIPS/MYPIPS/prod/newgen/TAGS"
     "/home/keryell/projets/PIPS/MYPIPS/prod/linear/TAGS"
     "/home/keryell/projets/PIPS/MYPIPS/prod/pips/TAGS")))
 )
\end{verbatim}
\item for a development version of the tags :
\begin{verbatim}
(custom-set-variables
  ;; ...
 '(tags-table-list (quote
    ("/home/keryell/projets/PIPS/svn/newgen/trunk/TAGS"
     "/home/keryell/projets/PIPS/svn/linear/trunk/TAGS"
     "/home/keryell/projets/PIPS/svn/pips/trunk/TAGS")))
 )
\end{verbatim}
\end{itemize}


\subsection{Developing PIPS under SVN}
\label{sec:dev-svn}

A basic understanding of svn operations and concepts such as
\emph{repository, checkout, status, commit} are necessary to develop in
PIPS.  See for instance the SVN book at
\LINK{http://svnbook.red-bean.com/} and other resources on the Internet.

The development policy within the PIPS-related software is that the
\texttt{trunk} version should hold a production quality software which
should pass all non-regression tests that run nightly at CRI. Thus day-to-day
development should not occur within this part, but in independent
per-developer \texttt{branches} which should be validated before being
merged back into the \texttt{trunk}.


\subsubsection{Pips development branches}

In order to help with the branch management, we propose the following
procedure based on the \texttt{svn\_branch.sh} script provided in
\texttt{pips/trunk/makes}.

\begin{enumerate}
\item Obtain an svn user account on \texttt{svn.cri.ensmp.fr}
  by asking for it to Fabien Coelho.
  For instance, a login \texttt{calvin} and some password may be attributed.
  Calvin will have his own development area under \texttt{branches/calvin}
  in the subversion repository.

\item Extract your own PIPS with the setup script (\texttt{setup\_pips.sh})
  mentionned above.
  The resulting directory may contain at the end of the building process:
  \begin{description}
  \item[prod] compiled production version of \texttt{pips},
    \texttt{newgen}, \texttt{linear} and the external \texttt{polylib}
    library;

    %% should the dev directory be called calvin?
    %% however how to differentiate newgen/linear/pips branches in such cases?
  \item[pips\_dev] Calvin's private development area which points to
    \texttt{branches/calvin} and should be empty at the beginning;
    It can be created afterwards with a manual checkout if necessary.
{\small
\begin{verbatim}
sh> svn co http://svn.cri.ensmp.fr/svn/pips/branches/calvin pips_dev
\end{verbatim}
}
    Branches may also be used for linear and newgen development,
    in which case the local directory name for the checkout should be called
    \texttt{linear\_dev} or \texttt{newgen\_dev} respectively.
  \item[valid] a working copy of pips non regression tests for validation.
    See Section~\ref{sec:utils} for informations about the validation,
    and then really read the \texttt{README} file to run it.
  \item[pipsrc.sh] sh-compatible shell initialization;
  \item[pipsrc.csh] csh-compatible shell initialization.
  \end{description}

\item Create your own development branch refering to a given library directory
  in the production version. Assuming that Calvin wants to develop
  in the \texttt{hpfc} library in pips:
\begin{verbatim}
sh> cd /home/.../MYPIPS/prod/pips/src/Libs/hpfc
sh> make SVN_USERNAME=calvin create-branch
# ... not really created before the commit
sh> svn commit /tmp/svn_branch.XXXXXX/
\end{verbatim}
  It creates a new branch, not yet committed, in some temporary directory.
  \verb/SVN_USERNAME/ is not necessary if the svn and unix account share the
  same login name. This directory \textbf{must} then be committed.
\item If necessary, update your development area to make the branch appear.
\begin{verbatim}
sh> cd /home/.../MYPIPS/pips_dev
sh> svn update
A    hpfc
sh> cd hpfc
\end{verbatim}
\item Develop, test, commit within your branch\ldots
  Some makefile targets of interest to the developer:
  \begin{description}
  \item[diff] svn diff of local directory.
  \item[info] svn info of local directory.
  \item[branch-diff] what you have developed in your branch, i.e. the
    differences between your branch and the production version
    (\texttt{trunk}).
  \item[branch-info] information about your branch.
  \item[branch-avail] list of commits on the trunk that where not pulled
    into the branch.
  \end{description}

  It may be desirable to look for the differences between the branch
  and trunk with additional options, for instance to ignore space changes.
  In order to do that:
  \begin{enumerate}
  \item get the branch initial revision with \texttt{make branch-info}.
    Let us assume that the result is 12345.
  \item perform a svn diff: {\small for example by passing through the
      \texttt{-x} option the \texttt{-b} option to \texttt{diff} to ignore
      changes in the amount of white space:
\begin{verbatim}
sh> svn diff --revision 12345:HEAD -x -b
\end{verbatim}
      or with the \texttt{-w} option to ignore all white space:
\begin{verbatim}
sh> svn diff --revision 12345:HEAD -x -w
\end{verbatim}
}
  \end{enumerate}

\item If you want to merge into your branch the on-going development from
  the trunk, see \S~\ref{sec:other-approaches} to do a \texttt{pull}.

\item When you are done with your developments, they are committed into
  your branch, you want to install them back to the pips main line.
  You must have write access to the \texttt{trunk} in order to do the commit.
{\small
\begin{verbatim}
sh> make install-branch
# ... create a temporary merged library
# ... there may be conflicts to be dealt with
sh> cd /tmp/svn_branch.XXXXXX
sh> svn commit .
\end{verbatim}
}
  The \texttt{XXXXXX} part in the directory name is some temporary number
  created for the nonce by the branch script.

  There are two commits:
  \begin{itemize}
  \item the first one is in trunk to install the new version of the library.
  \item the second one in the branch to update bookeeping informations stored
    within the branch.
  \end{itemize}

  It may be possible (not tried) to commit several libraries at once.
\end{enumerate}


\subsubsection{Other approaches}
\label{sec:other-approaches}

You may also decide that the previous procedure is not for you. Well, it
may not be advisable to begin with, but it can be useful to work on many
libraries in PIPS at the same time and to have an atomic view or even
having different versions of your own in your branch, for example.

Here are other possible approaches for developping in PIPS,
that you may use provided that you do not break the development
policy which requires all changes committed to the \texttt{trunk}
not to break the non-regression tests.

It is possible to use the \texttt{svn\_branch.sh} script directly to
create a branch for any part of pips, including the full
software. \verb|svn_branch.sh| has been developed because \textsc{svn}
cannot yet deal natively with branching but it is planned to change when
all the PIPS developers will use SVN version 1.5 or more.

It is possible to use \texttt{svn} directly to create and manage your
branches with copies. The \texttt{svn\_branch.sh} adds convenient
administrative data in the directory to know about the status of the
branch: what is its source, what was merged (pulled) from the source or
joined back (pushed) to it, and so on. These informations are stored in
\texttt{svnbranch:...} properties. So if you want to play around with
home-made branching, think about them too...

Here is an excerpt of the available \verb|svn_branch.sh| that can be
described more precisely with \verb|svn_branch.sh --help|
\begin{description}
\item[\texttt{create}] a new branch of something somewhere;
\item[\texttt{pull}] into your branch the modifications that have been
  done in the trunk since your last \texttt{pull} or branch creation;
\item[\texttt{push}] merge back your branch into the trunk since the last
  merge you've done.
\end{description}

If you want to directly hack the SVN repository, for example, if you want
to merge directly and atomically into your own working copy of the trunk
your own copy of several branch pieces such as
\texttt{pips/trunk/src/Libs/expressions} that is from the trunk at
revision 12475 and so on, you can do something like:
\begin{verbatim}
cd .../svn/pips/trunk/src/Libs/expressions
svn merge -r 12475:HEAD ../../../../branches/godet/expressions
cd .../svn/pips/trunk/src/Libs/sac
svn merge -r 12356:HEAD ../../../../branches/godet/sac
cd .../svn/pips/trunk/src/Libs/phrase
svn merge -r 12356:HEAD ../../../../branches/godet/phrase
cd cd .../svn/pips/trunk
svn commit
\end{verbatim}
But it should be used only for emergency case (for example if the branch
were developed outside the PIPS branch infrastructure).

It is finally possible to edit directly your production working copy, and
just commit from there when you're happy with your changes.  This may be
advisable for small easy to test global changes, such as adding a new
library in PIPS. This is not advisable for long development, as it is
often useful to have an history of such developments.


\subsubsection{Developing in a branch? Validate before merging!}
\label{sec:devel-branch-valid}

Do not forget that if you or someone else merges your branch into the
trunk, things must be validated against the global PIPS validation to
avoid havoc for the other PIPS developers.

Here is a typical behaviour you should follow:
\begin{enumerate}
\item update your production copy of PIPS (the \texttt{trunk}) with
  \texttt{svn up}, recompile the production version and apply the
  validation against it;
\item commit your branch if not already done;
\item merge into your branch the last developments from the trunk with a
  \texttt{svn\_branch.sh pull}, recompile (so it will modify the
  executables and libraries in the production version) and validate;
\item if it is worse than point 1, improve the code in your branch and go
  to step 1 again;
\item if you are happy with the validation, merge your beautiful work into
  the PIPS Holly trunk with \texttt{svn\_branch.sh push} so that all the
  PIPS community can congratulate you.
\end{enumerate}

For more information about the validation process, have a look to
\S~\ref{sec:validation}. Since your validation mileage may vary according
to your target architecture because of bugs or limitations (little or big
Endian, native 32-bit or 64 bit architecture...) you should try to
validate on various architectures.


\subsubsection{Understanding the Makefile infrastructure inside the SVN
  infrastructure}
\label{sec:underst-makef-infr}

This section is to be skipped at first reading and is only of interest for
people wanting to modify more globally the PIPS infrastructure.

The compilation directories are managed by some global variables
initialized as described in section~\ref{sec:shell}.

If you compile into a directory, in the \verb/prod/ directory installed by
the installation script, your branch directory or whatever, the compiled
stuff from a directory goes to \verb/$NEWGEN_ROOT/, \verb/$LINEAR_ROOT/
\verb/PIPS_ROOT/ respectively, use the Makefile definitions from their
\verb/$/\emph{*}\verb/_ROOT/ and are build with library found in the other
\verb/$/\emph{*}\verb/_ROOT/ directory. The Makefile parts are picked into
\verb/$/\emph{*}\verb|_ROOT/makes| directory, \emph{except} for
\emph{Makefile} that are local symbolic links.
%$

For example, if you have a local PIPS copy in the \texttt{svn/pips/trunk}
and you compile with \texttt{make}, it will build against
\verb/$NEWGEN_ROOT/ and \verb/$LINEAR_ROOT/, it will install into
\verb/$PIPS_ROOT/ and will use \texttt{Makefile} stuff from
\verb|$PIPS_ROOT/makes|, and only few forwarding \texttt{Makefile} are
sym-linked to \texttt{svn/pips/trunk/makes}. So if you want to modify the
\texttt{Makefile} infrastructure, modify mainly \verb|$PIPS_ROOT/makes| %$
and commit it. It will be commited into the \texttt{svn/nlpmake/trunk}
repository anyway.


\section{Shell environment (sh, ksh, bash, csh, tcsh)}
\label{sec:shell}

Many environment variables may be used by PIPS executables and utilities. 
They mainly describe PIPS directory hierarchy, compilation options, helper
programs, and so on.

These variables should be initialized by sourcing the 
\texttt{pipsrc.sh} file for \texttt{sh ksh bash} shells,
or \texttt{pipsrc.csh} file for \texttt{csh tcsh} shells. 
An initial version of these files is created by the setup script, 
and can be customized as desired.

Two variables are of special interest: \texttt{\$PIPS\_ROOT} 
which stores the root of PIPS current version, and \texttt{\$PIPS\_ARCH}.

\subsection{PIPS architecture (\texttt{PIPS\_ARCH})}

The variable holds the current architecture. If not set, 
a default is automatically derived with the \texttt{arch.sh} script. 

A PIPS architecture is to be understood not as strictly as a computer
architecture. It simply a set of tools (compilers, linkers, but also
compiler options\ldots) to be used for compiling PIPS. Thus you can have
several pips architectures that compile and run on a very same machine.

This set of tools is defined in the corresponding
\texttt{\$PIPS\_ARCH.mk} makefile,
where usual \texttt{CC}, \texttt{CFLAGS} and so make macros are defined.

This configuration file is automatically included by all makefiles, hence
changing this variable results in different compilers and options to be
used when compiling or running pips. Object files, libraries
and binaries related to different pips architecture cannot be mixed and
overwritten one by the other: they are stored in a subdirectory depending
on the architecture, \emph{à la} PVM. Thus pips versions are always
compiled and linked with libraries compiled for the same architecture.

Here are examples of pips architectures:
\begin{description}
\item[.]: default version used locally, so as to be compatible with the
 past. \texttt{gcc}, \texttt{flex} and \texttt{bison} are used.
\item[DEFAULT]: default compilers and options expected to run on any
  machine (\texttt{CC=cc}, and so on). The C compiler is expected to support
  ANSI features, includes and so.
\item[GNU]: prefer gnu tools, as \texttt{gcc flex bison g77}.
\item[SUN4]: SUN SUNOS 4 compilers \texttt{acc lex yacc f77} etc.
\item[GNUSOL2LL]: version for SUN Solaris 2 compiled with gnu softwares and
  using ``long long'' (64 bits) integers in the mathematical computations
  of the C3/Linear library.
\item[GPROF]: a gnu version compiled with -pg (which generates a trace
  file that can be exploited by \texttt{gprof} for extracting profiling
  information)
\item[IBMAIX]: the compilers and options used on IBM AIX workstations. 
\item[LINUXI86]: for x86 machines under linux.
\item[LINUXI86LL]: for x86 machines under linux with long long integers.
\item[LINUX\_x86\_64\_LL]: for 64-bit x86 machines under linux with long long
  integers.
\item[OSF1]: for DEC Alpha machines under OSF1. 
\end{description}


\section{PIPS Project directories}
\label{sec:directories}

This section describes the PIPS directory hierarchy. 
When extracting a repository with the \texttt{setup\_pips.sh} script,
three subprojects are extracted: \texttt{newgen} (software engineering tool,
Section~\ref{sec:newgen}),
\texttt{linear} (mathematical library, Section~\ref{sec:linear}) 
and \texttt{pips} (the project).
The three projects are organized in a similar way, with a \texttt{makes}
directory with makefiles and a \texttt{src} directory for the sources.
Other directories \texttt{bin share runtime doc} are generated on
compilation (\texttt{make compile} or \texttt{make build}).

\subsection{The \texttt{pips} directory}

This directory contains the current version, namely pips subversion 
\texttt{trunk} directory. 
The \texttt{\$PIPS\_ROOT} environment variable should point to this directory.
You can find the following subdirectories:

\subsubsection{The \texttt{makes} subdirectory}

PIPS makefiles, and some helper scripts.

\subsubsection{The \texttt{src} subdirectory}

The PIPS Sources are here.

Having a copy of this subtree and of the makes directory is enough for fully
recompiling PIPS, including the documentation, configuration files and
various scripts used for running and developping the PIPS sofware.

\begin{description}
\item[src/Documentation]:
  This directory contains the sources of the documentation of
  PIPS. From some of these are derived automatically header and
  configuration files.

  The \texttt{newgen} sub-directory contains the description (in \LaTeX{})
  of the internal data structures used in the project, as they are used
  in the development and production hierarchies.
  %%
  The local makefile transforms the \texttt{*.tex} files
  describing PIPS data structures into NewGen and header files, and
  exports them to the \texttt{include} directory. %%$
  Thus the documentation actually \emph{is} the sources for the data
  structures.

  The \texttt{wpips-epips-user-manual} sub-directory contains the user
  manual.

  The \texttt{pipsmake} sub-directory contains the definition of the
  dependences between the different interprocedural analyses as a
  \LaTeX{} file, from which are derived the \texttt{pipsmake.rc}
  configuration file used by PIPS at runtime.

\item[src/Passes]: This directory contains the sources of the
  different passes, in several sub-directories named from the passes
  (\texttt{pips tpips wpips fpips}). If you add a pass, it is
  mandatory that the name of the directory is the name of the pass.

\item[src/Libs]: This directory contains the source of the several
  libraries of PIPS\@.  It is divided into sub-directories named from
  the libraries. The name of the subdirectory must be the name of the
  library.

\item[src/Scripts]:
  This directory contains the source of the shell scripts which are
  useful for PIPS, the linear library and NewGen. It is further
  divided into several directories. In each sub-directory a local
  \texttt{Makefile} performs automatic tasks such as the installation of
  the sources where expected (usually in \texttt{utils}),
  depending whether the scripts is used for
  \emph{running} or \emph{developing} PIPS.

\item[src/Runtimes]: This directory contains the sources of the
  libraries used when executing codes generated by PIPS; in fact,
  there are only two libraries: The first for the HPF compiler
  \texttt{hpfc}, and the second one for the WP65 project.
\end{description}

\subsubsection{The \texttt{bin} subdirectory}

Pips current binaries that can be executed. 

These binaries include \texttt{pips} (the main program), 
\texttt{tpips} (the same program with a interactive interface based on the 
GNU readline library) and \texttt{wpips} (the window interface). 

Non architecture-dependent executables needed or used by pips
are also available, such as old \texttt{Init Select Perform Display Pips}
pips shell interfaces, and \texttt{pips tpips wpips epips jepips} launchers.
  
Actual architecture-dependent binaries are stored in
different sub-directories depending on \texttt{\$PIPS\_ARCH}
which describes the current architecture, as discussed in
Section~\ref{sec:shell}. 

\subsubsection{The \texttt{etc} subdirectory}

Configuration files, such as properties and pipsmake settings.

Also some configuration files, automatically generated from
the \LaTeX{} documentation in \texttt{src/Documentation}.
Important files are \texttt{pipsmake.rc}, \texttt{properties.rc} and
\texttt{wpips.rc}.

\subsubsection{The \texttt{share} subdirectory}

They include shell (sh and csh), sed, awk, perl and lisp scripts.


\subsubsection{The \texttt{doc} subdirectory}

Pips documentation, namely many PDF files describing various
aspects of PIPS, the internal data structures and so on.  Important
documents: \texttt{pipsmake-rc} (the dependence rules between pips
interprocedural analyses), \texttt{developer\_guide} (this document!),
\texttt{ri} (the description of the pips Intermediate Representation, also
known as the Abstract Syntax Tree (\textsc{ast})).

This directory populated from the various \texttt{src} directories and is
also used to build the static PIPS home page.

%% \subsubsection{man}
%% Local (obsolete?) man pages.

\subsubsection{The \texttt{html} subdirectory}

This HTML documentation of PIPS is populated from the various \texttt{src}
directories. There are real HTML files, and some generated from \LaTeX{}
files.

This directory is also used to build the static PIPS home page.


\subsubsection{The \texttt{runtime} subdirectory}

Environments needed for executing pips-compiled files.  Namely
\texttt{hpfc} and \texttt{wp65} use a PVM-based runtime and also
\texttt{xPOMP} the graphical user programming interface.  This directory
includes the files needed for executing the generated files (as header and
make files or compiled libraries), but not necessarily the corresponding
sources.

\subsubsection{The \texttt{utils} subdirectory}
\label{sec:utils}

Other architecture independent tools and files used for \emph{developing}
pips (as opposed to \emph{running} it), such as validating PIPS, dealing
with the SVN organization.

But there are also some scripts used to help PIPS usage, such as
displaying some graphs or dealing with PIPS output.


\subsubsection{include}

This directory contains all shared files needed for building PIPS.  It
includes C header files automatically generated from NewGen specifications
and for each library, and from some file in the documentation. Also pips
architecture configuration file that define makefile macros are there.

\subsubsection{lib}

This directory contains PIPS compiled libraries ({\tt lib*.a}) ready
to be linked. They are stored under their \texttt{\$PIPS\_ARCH}
subdirectory.

\section{Makefiles}
\label{sec:makefiles}

The GNU {\tt make} utility is extensively used to ensure the coherency of
PIPS components. However, most {\tt Makefile}s are not written by hand,
but are automatically composed according components from \texttt{nlpmake}
package and automatic dependence computation. It eases development a lot,
by providing a homogeneous environment all over the PIPS software parts.

If you need some specific rules or targets, you have to write some
makefile whith named ending with the \texttt{.mk} extension: they will be
included in the global \texttt{Makefile}.

By default, PIPS \texttt{make} does not stop on error but reports them on
standard error output. But if you want to have \texttt{make} stopping on
the first error to help debugging, just add \verb|FWD_STOP_ON_ERROR=1| to
the \texttt{make} invocation.

The rationale for relying on GNU make rather than make is that in the
previous situation the software was relying heavily on SUN make special
features, thus was highly not portable. Now it is more portable, say as
much as the GNU softwares. The complex makefiles of PIPS rely on GNU
extensions such as conditionals for instance. 

\subsection{Global targets}
\label{sec:global-targets}

Global targets are targets useful at the top of PIPS to with global
meaning such as building a global binary file from the sources.

Useful targets defined:
\begin{description}
\item[build:] generate a running version with the documentation;
\item[compile \& recompile:] generate a running (hope so...) version. It is the default
  target to \texttt{make} if you don't precise one;
\item[doc:] build the documentation (such as the one you are reading right
  now);
\item[clean:] clean up the project directories;
\item[local-clean:] remove eventually some stuff in the current directory;
\item[tags:] build the TAGS index files of identifiers found in the
  subdirectories (see~\ref{subsec:tags} for more precision and other
  specific targets);
\item[unbuild:] clean up things: generated directories are removed;
\item[htdoc:] generate an HTML version of the documentation
\item[full-build:] like \texttt{build} but with \texttt{htdoc} too.
\end{description}


\subsection{Local targets}
\label{sec:local-targets}

Local targets are actions with a more local meaning to help developing
some passes or documentation parts, such as:
\begin{description}
\item[\texttt{local-clean}:] remove eventually some stuff in the current
  directory;
\item[\texttt{depend}:] \texttt{make depend}\index{make!depend} must be
  used regularly to keep file dependencies up-to-date. The local header
  file corresponding to the current pass or library must have been
  created\marginpar{Is this line obsolete?} before, otherwise \texttt{make
    depend} selects the header file from \verb+$PIPS_ROOT/include+, and
  the local header will never be created;%% $
\item[\texttt{fast}:] {\tt make fast}\index{make!fast} recompile
  the local library and relink the executables.
\item[\texttt{full}:] {\tt make full}\index{make!full} rebuild PIPS
  fully from the root, so that all rependencies are checked.
\item[\texttt{compile}:] also builds the documentation;
\item[\texttt{full-compile}:] also builds the documentation for web publication;
\end{description}
\marginpar{There are other rules to describe\ldots}

THIS PARAGRAPH IS OBSOLETE.\marginpar{I cannot see how to compile without
  installing to the PROD directory...}
{\tt make libxxx.a} just recompiles the local {\tt *.c} files, and creates
the local library file {\tt libxxx.a}. This is useful when making a lot of
changes to check the syntax; or when making changes in several directories
at the same time: in this case, the proper policy is to a) chose one of these
directories as the master directory, b) build the {\tt libxxx.a} in the other
directories, c) make symbolic links towards them in from the master
directory, d) and finally link in this last directory. If your pips
architecture is not \verb|.|, don't forget to link the proper libraries
in the proper sub-directory.

A typical example of a local \texttt{Makefile}\index{config.makefile} file
is provided in Figure~\ref{fig:config_makefile}.

\begin{figure}
\begin{verbatim}
# The following macros define your pass:
TARGET	= rice

# Sources used to build the target
LIB_CFILES =	rice.c codegen.c scc.c icm.c

# Headers used to build the target
INC_TARGET =    $(TARGET).h

LIB_TARGET =    lib$(TARGET).a

# common stuff
ifdef PIPS_ROOT
ROOT    = $(PIPS_ROOT)
else
ROOT    = ../../..
endif

PROJECT = pips
include $(ROOT)/makes/main.mk
\end{verbatim}
  \caption{Example of \texttt{Makefile}.}
  \label{fig:config_makefile}
\end{figure}



\section{Validation}
\label{sec:validation}

The validation is stored in a separate subversion repository
\url{http://svn.cri.ensmp.fr/svn/validation}

Different phases are validated in distinct sub-directories.
Running the validation uses the \texttt{pips\_validate} script.

See the \texttt{README} at the root of the \texttt{trunk} for details on
how to run the validation. Basically, one must simply type \texttt{make}.

See various examples around to add new validation files.  The preferred
way is to provide a source file \texttt{MyTest.f} or \texttt{MyTest.c}, a
corresponding \texttt{MyTest.tpips} script, and store the standard output
of the \texttt{.tpips} script into \texttt{MyTest.result/out}.

The output \texttt{MyTest.result/out} is then compared against the
reference \texttt{MyTest.result/test} to know if the validation is
correct. So to install a new validation item, you need to create the
reference \texttt{MyTest.result/test}. \texttt{pips\_validate -a} (see
afterwards) can help to install a new validation reference file and
directory.

For more complex validations, a \texttt{.tpips} script without a direct
corresponding source file but that may use many files from various
location is also possible.

\textbf{CAUTION:} use a relative and local path in the \texttt{.tpips}
file to reference the source file.

To do more subtle things (nightly validation with mail notification, etc)
it is possible to use validation scripts instead.

But it is best \textbf{not} using the following validation scripts
directly, but rather to rely on the \texttt{Makefile} in the validation
directory, which has very convenient targets including \texttt{validate
  accept clean}.  Using this approach does not require the validation
directory to be under \texttt{\$PIPS\_ROOT}.

Two validation scripts are available:
\begin{description}
\item[pips\_validate]: \index{Validate} Non-regression tests;

  The arguments are names of subdirectories in
  \texttt{\$PIPS\_ROOT/../../validation} or source files within these
  directories.  See \texttt{pips\_validate -h} for help and a list of
  options.

  Tests are organized more or less on a library basis in \texttt{valid}.
  Non-regression tests are characterized by one or more source
  programs which must have a known result.

  Set \texttt{PIPSDBM\_DEBUG\_LEVEL} to 5 to check data structures when
  they are stored and to 9 to check them when they are delivered; some
  data structures are not checked, among them, all that contain
  external types;

  The validation is performed for each source file found in a directory.
  These validation scripts \textbf{must not} rely on absolute path,
  otherwise moving the validation around is impossible.
  %%
  If you really need to rely on the directory containing the scripts, use
  the \texttt{\$VDIR} environment variable that is initialized by the
  validation infrastructure, or better \texttt{\$\{VDIR:-.\}} to default
  to the current directory to test your scripts outside of the validation
  infrastructure.

  The scripts looks for validation scripts in the following order:
  \begin{enumerate}
  \item a corresponding \texttt{.test} file, and executes it.

  \item a \texttt{.tpips} file which is executed as a
    tpips script.

  \item a corresponding \texttt{.tpips2} file which is executed as a
    tpips script, with the error stream redirect.

    Validating the error stream is a \textbf{very bad idea} as it leads
    to never ending regression variations. It is even worth when the
    output comes from external tools such as compilers error output.

  \item a \texttt{default\_test} file as a template for a \texttt{.test} file.

    First it performs some substitutions
    (\texttt{tested\_file}, \texttt{tested\_dir}, \texttt{tested\_name}
    and \texttt{TESTED\_NAME} are substituted by the full file name,
    the validation directory, the name prefix in lower or upper)
    and then it executes the generated test file.

  \item a \texttt{default\_tpips} file and runs it with \textsc{tpips}.

    The environment variables \texttt{FILE} and \texttt{WSPACE} contain
    the full-path file name and the file name prefix.

  \item if none of these cases apply, the scripts parallelize all modules.
  \end{enumerate}

  For maintenance purposes, the \texttt{default\_tpips} approach is the
  best choice (only one source file to maintain for all the sources of a
  validation directory). However, this can be used only if the very same
  test is to be applied. As a second choice, the \texttt{.tpips} script is
  encouraged. It is very important not to validate \texttt{stderr} output
  and pips with some level of debugging on, because these outputs are not
  very stable from one architecture to another.

\item[manual\_accept] \index{Validate} interactive script to accept new
  validation results. It accepts a list of directories to ask for
  accepting or it deals with all the validation by default.

  Once you have accepts the new states as a reference for some validation
  items, you need to commit these changes to the validation repository, if
  and only if the corresponding pips sources are already in the subversion
  repository of course!

  If you are Emacs oriented, setting the \verb|PIPS_GRAPHICAL_DIFF|
  environment variable to \verb|pips_emacs_diff| to use graphical Ediff
  mode sounds like a good idea.
\end{description}

If you work on a branch, you should also have a look to
\S~\ref{sec:devel-branch-valid}.

%%\begin{itemize}
%%\item \verb+bug-to-validate+: \index{bug-to-validate}When a bug is
%%  corrected, run \verb+bug-to-validate+ to move the bug case from
%%  \verb+Bugs/xxx+ to \verb+Validation/xxx+.
%%  Do not forget to update \verb+Documentation/pips-bugs.ftex+.
%%  
%%\item \verb+pips-experiment+: \index{pips-experiment}runs the same
%%  analyses and transformations on each module in a workspace.
%%  
%%\item \verb+print-dg-statistics+: \index{print-dg-statistics}exploits some
%%  of the statistics generated by the dependence graph computation when a
%%  specific property is set to {\tt TRUE}
%%  (\verb+RICE_DG_PROVIDE_STATISTICS+). 
%%  
%%\item there many other things in this directory; volunteers to write their
%%  documentation here are welcome!
%%\end{itemize}


\section{Debugging and NewGen}
\label{sec:newgen}

NewGen is a software engineering tool, from which almost all PIPS data
structures are generated. It is described in ???. It is considered as an
external tool, and is independent from the PIPS project. However, it is
required for building pips from scratch, and the Newgen runtime library
(genC) must be linked to pips.

NewGen sources are in the directory \verb+$NEWGEN_DIR+. %% $
The global organization is similar to the PIPS organization.

\verb+$NEWGEN_ROOT+ and \verb+$NEWGEN_ARCH+ refer to the current version
and architecture to be used. If \verb+$NEWGEN_ARCH+ is not defined, is
defaults to \verb+$PIPS_ARCH+ and then to ``\verb+.+'', so that if one
develop or update NewGen as part of PIPS there is no trouble.

It may (alas!) sometimes be useful to consult them. Several
functionalities are useful when debugging:
\begin{itemize}
\item \verb+gen_consistent_p()+ \index{gen\_consistent\_p}checks whether
  an existing NewGen data structure is (recursively) well formed.
\item \verb+gen_defined_p()+ \index{gen\_defined\_p}checks whether an
  existing NewGen data structure is (recursively) completely defined.
\item \verb+gen_debug+ \index{gen\_debug}is an external variable to
  dynamically check the coherence of NewGen data structures.

  Activation:
  \begin{quote}
    \verb+ gen_debug |= GEN_DBG_CHECK;+
  \end{quote}
  Desactivation:
  \begin{quote}
    \verb+ gen_debug &= ~GEN_DBG_CHECK;+
  \end{quote}
  And when all else fails:
  \begin{quote}
    \verb+ gen_debug = GEN_TRAV_OBJ+
  \end{quote}
\item To print the type (the domain) number of a NewGen object under
  \verb+dbx+ or \verb+gdb+:
  \begin{quote}
    \verb|print obj->_type_|
  \end{quote}
\item \item To print the type (domain) name of a NewGen Object from its
  number:
  \begin{quote}
    \verb|p Domains[obj->_type_].name|
  \end{quote}
\item To directly dig into a NewGen object, you can access an attribute
  \texttt{a} of an object \texttt{obj} of type \texttt{type} with its
  field name \verb|_type_a_|, thus such as \verb|obj->_type_a_|.

  For example, picked from the RI:
  \begin{itemize}
  \item To print the name of an entity \verb+e+ (an entity is a PIPS data
    structure, see \verb+ri.tex+):
    \begin{quote}
      \verb|p e->_entity_name_|
    \end{quote}
  \item To print the label of a statement \verb+stmt+, by studying the RI
    you can see that:
\begin{verbatim}
statement = label:entity x number:int x ordering:int
          x comments:string x instruction:instruction
          x declarations:entity* x decls_text:string;
\end{verbatim}
    and since a \texttt{label} is an \texttt{entity} as declared with:
\begin{verbatim}
tabulated entity = name:string x type x storage x initial:value
\end{verbatim}
    you can get the information you want with:
    \begin{quote}
      \verb|print stmt->_statement_label_->_entity_name_|
    \end{quote}
  \end{itemize}
\item To print a statement \texttt{s} as human code, \texttt{call
    print\_statement(s)}. You may need to position some internal variable
  to have the correct meaning, such as if you are dealing with C program
  with:
\begin{verbatim}
set is_fortran=0
\end{verbatim}
\item To print an expression \texttt{e} as human code, use
\begin{verbatim}
call print_expression(e)
\end{verbatim}
\end{itemize}

\marginpar{Fabien: NewGen should be simplified to use
  \texttt{stmt->label->name}}

It should be also possible to use PIPS and NewGen macros (accessors and so
on) from GDB instead of these NewGen internals, if the sources are
compiled with the good options (\texttt{-ggdb -g3}). You may remove the
\texttt{-O} option from the \texttt{CFLAGS} in the makefiles to avoid
variable optimizations into registers. A good and simple practice to
achieve this is to use:
\begin{verbatim}
make CFLAGS='-ggdb -g3 -Wall'
\end{verbatim}

In the future, {\tt gdb} functions compatible with Newgen generated C
macros will be provided\footnote{Well, we just need GNU people to add
  interactive functions to \texttt{gdb} and then we will provide the
  macros.}.

To ease this kind of inspection, some keyboard accelerators can be defined
inside the Emacs debugger mode by adding into you \texttt{.emacs}such as:
\begin{verbatim}
(add-hook 'gdb-mode-hook
 (function
  (lambda ()
   (gud-def pips-display-type "print Domains[%e->_type_].name" "t"
            "Display the domain (that is the type) of the selected NewGen object")
   (gud-def pips-display-entity-name "print %e->_entity_name_" "n"
            "Display the name of the selected entity")
   (gud-def gud-affiche-expression "call print_expression(%e)" "e"
            "Print a PIPS expression")
   (gud-def gud-affiche-statement "call print_statement(%e)" "s"
            "Print a PIPS statement")
   (setq
    ;; Use Emacs with many GDB buffers as an IDE:
    gdb-many-windows t
    ;; Don't mix up gdb output with PIPS output
    gdb-use-separate-io-buffer t
    )
   )
  )
 )
\end{verbatim}
These function are then available with the \verb|C-x C-a|
binding, so you click on an interesting variable and then use
\begin{description}
  \item[\texttt{C-x C-a t}] to display the type of a NewGen object;
  \item[\texttt{C-x C-a n}] to display the name of a PIPS entity;
  \item[\texttt{C-x C-a e}] to display the type of a PIPS expression;
  \item[\texttt{C-x C-a s}] to display the type of a PIPS statement.
\end{description}



\section{Documentation}
\label{sec:documentation}

\subsection{Source documentation}
\label{sec:source-documentation}

The various documentations, including the source of the web pages, are
stored in the \texttt{src/Documentation} directory.

For example this guide is stored in \verb|src/Documentation/dev_guide|.

Have a look also on \S~\ref{sec:brows-source-files} about Doxygen
documentation of the sources.


\subsection{Web site}
\label{sec:web-site}

The various PIPS documentations can be published on the web site
\PipsOldWWW with the \verb|pips_publish_www| once you have compiled it.

To publish the Doxygen documentation of the sources, as explained in
\S~\ref{sec:brows-source-files}, once you have compiled the Doxygen
version, use a \texttt{make publish} to push the files on the PIPS Doxygen
server.

There is also a new dynamic web site \PipsNewWWW that reference better the
information stored on\PipsOldWWW. The idea is to have the content edited
naturally and collaboratively onto \PipsNewWWW and from time to time
injecting back the content into the \PipsOldWWW by editing the
\texttt{src/Documentation/web} directory.


\section{Library internal organization}
\label{sec:library_internal_organization}

The source files for the library \texttt{\emph{L}} are in the directory
\verb|$PIPS_ROOT/src/Libs/|\texttt{\emph{L}}. %$

A \texttt{Makefile} must be created in this directory. It can be copied from an
existing library since it ends with PIPS special include instructions used
to simplify the make infrastructure, but it must be updated with the names
of the local source and object files. Additionnal rules compatible with
the \texttt{make} syntax can also be added in this \texttt{Makefile},
but now the preferred method is
to put them in \texttt{local.mk} as explained later. For local
\textsc{lex} and \textsc{yacc} files, use the \verb|$(SCAN)| and
\verb|$(PARSE)| macros, and do not forget to change the 'yy' prefixes
through some sed script or better with the correct parser generator
options to avoid name clashes when linking PIPS (which already includes
several parsers and lexers).

The {\tt Makefile} calls the PIPS make infrastructure to define
for example a default \texttt{recompile} entry to install
the library in the production hierarchy, typically in
\verb|$PIPS_ROOT/include| and \verb|$PIPS_ROOT/lib|.

Great care must be taken when creating the library header file
\texttt{\emph{L}.h}.  This file includes the local header file
\texttt{\emph{L}-local.h} (which contains macros, type
definitions\dots) written by the programmer and the external functions
declared in the local source files, which are automatically extracted
using {\tt cproto}.

\texttt{\emph{L}.h} must never be directly modified as indicated by the
warning at the beginning. It is automatically generated when invoking {\tt
  make header}\index{make!header}, or when \texttt{\emph{L}-local.h} has
been modified, but not when the source files are modified. This avoids
inopportune recompilations when tuning the library, but can lead to
coherency problems.

If a main program to test or use the library exists, then it is
necessarily a pass (see section~\ref{sec:pass_organization}), or a
\lstinline|main()|. But the easiest way to test a library is to test
through the PIPS infrastructure after declaring it
as explained in section~\label{sec:direct-pipsmake}.


\subsection{Libraries and data structures}
\label{subsec:libraries_and_data_structures}


PIPS data structures are managed by NewGen. For each data structure
declaration file in \verb+$PIPS_ROOT/src/Documentation/newgen+, NewGen
derives a header file that defines the object interfaces, which is placed
in \verb+$PIPS_ROOT/include+.

For instance, the internal representation (i.e. the abstract syntax tree)
is called \verb+ri+\index{ri}\footnote{For the French «
  \emph{Représentation Interne} » or Internal Representation.}. It is
described in the \LaTeX\ file {\tt ri.tex} in
\verb+$PIPS_ROOT/src/Documentation/newgen+. It is then automatically
transformed into a NewGen file {\tt ri.newgen}. And finally, NewGen
produces an internal description file {\tt ri.spec}, a C file with the
object method definitions and a header file {\tt ri.h}, which must be
included in each C file using the internal representation.

Thus, it is not possible to build a new library with the name {\tt ri},
because the corresponding header file would be called {\tt ri.h}. The
library in which higher order functions concerning the {\tt ri} are placed,
is rather called {\tt ri-util}. It should be the case for all the other sets
of data structures specified using NewGen. Such existing libraries are
{\tt text-util} and {\tt paf-util}. However, mainly for historical
reasons, there are numerous exceptions. Many functions are in fact in the
library where they were first necessary. An example is the {\tt syntax}
library (the parser). It should be refactored out some day.



\subsection{Library dependencies}

Library dependence cycles must be avoided. Links are not easy to manage
when a module $a$ from library $A$ calls a module $b$ form library $B$
which itself calls a module $a'$ from library $A$. This situation also
reflects a bad logical organization of libraries. Therefore, PIPS
libraries must be organized as a DAG, i.e. they must have a partial
order. With modern linkers, it is no longer an issue but we still have the
issue at the \texttt{.h} level if we have cross-dependencies such as
cross-recursion.

Several utilities, such as \texttt{analyze\_libraries},
\texttt{order\_libraries}, \verb|grep_libraries|,
\verb|order_libraries_from_include| (see
\verb|$PIPS_ROOT/src/Scripts/dev|), can be used to determine the
dependences between libraries, a total order compatible with the library
partial order, and the possible cycles. The results are stored in
\texttt{/tmp/libs.?} where the question mark stands for several values:
\begin{itemize}
  \item u: uses, modules used by each library.
  \item d: defs, modules defined by each library
  \item j: join between uses and defs
  \item o: order between libraries
  \item etc...
\end{itemize}


\subsection{Installation of a new phase (or library)}

Some libraries implement functionalities which are accessible by the users
via the \texttt{pipsmake} mechanism. For a good comprehension of the material
contained in this subsection, the reader is referred to the corresponding
documentation.

A PIPS phase \emph{MYPHASE} is implemented as a C function named
\texttt{myphase} with a single argument, which is the name of the module
(a function or procedure) to analyze. It returns a boolean value, which
indicates whether the computation has performed well.  It therefore
resembles the following dummy function:
\begin{lstlisting}
bool myphase(char *module_name)
{
  bool good_result_p = TRUE;

  /* some computation */
  ......
  return(good_result_p);
}
\end{lstlisting}
This phase is usually located in a library named \texttt{myphase}, but this is
not compulsory. It must set the global variables which are necessary for its
execution using \texttt{properties}, and acquire the necessary resources by
invoking \texttt{pipsdbm}. Directly invoking \texttt{pipsmake} is forbidden at
this level.

This section did not deal with the case where a new resource is
introduced. This is the object of the section~\ref{sec:dealing-with-new}
instead.

Here are now the steps to install a new library named \texttt{mylib}.

\subsubsection{In the directory \texttt{src/Libs}}
\label{sec:direct-textttsrcl-1}

\begin{itemize}
\item First, create a new directory \texttt{mylib} in \texttt{src/Libs};
\item add the new directory name to the
  \verb|$PIPS_ROOT/src/Libs/local.mk|, so that the recursive make will
  consider it.
\end{itemize}

\subsubsection{In the directory \texttt{src/Libs/mylib}}
\label{sec:direct-mylib}

\begin{itemize}
\item Add some \texttt{.c} files (at least the one where the code is for
  the C \texttt{boolean mylib(string name)} function, which corresponds to
  the phase to add, generally is);
\item create a \texttt{Makefile} (see examples in the other libraries).
  This file must list the sources of the library and the targets to be
  built, then it must include the common makefile
  \verb|$(ROOT)/makes/main.mk|;
\item Then run the commands \texttt{make}. The file \texttt{mylib.h} is
  installed in the \verb+$PIPS_ROOT/include+ directory, and
  \texttt{libmylib.a} in the \verb+$PIPS_ROOT/lib/$PIPS_ARCH+ directory.
  The library functions are now accessible by the other libraries and
  passes.

  Note that the \texttt{cproto} command is used to generate the library
  header file \texttt{mylib.h}. To force this file to be build again, do a
  \texttt{make header}.
  \end{itemize}


\subsubsection{In directory \texttt{\$PIPS\_ROOT/makes}}
\label{sec:direct-makes}

\begin{itemize}
\item Update file
  \texttt{define\_libraries.mk}\index{define\_libraries.mk}.  It contains
  the ordered list of PIPS libraries for linking.  The order in which the
  libraries are added is important: the new library must be added after
  the libraries which use its modules, but after the libraries whose
  modules it uses.\marginpar{RK to FC: I'm not sure it is still true for
    modern ld.}
\end{itemize}


\subsubsection{In directory \texttt{src/Scripts/env}}
\label{sec:direct-env}

If you need to introduce new environment variables or to modify old ones
(not likely):
\begin{itemize}
\item possibly update \texttt{pipsrc.sh}\index{pipsrc.sh} file.  It
  contains the shell environement variables for PIPS. Do not modify
  \texttt{pipsrc.csh}\index{pipsrc.csh} since it is automatically
  generated by the following item;
\item run the \texttt{make} command to automatically build the files and
  to install them in \verb|$PIPS_ROOT/share|.
\end{itemize}

Note that the \texttt{pipsrc.*sh} files created at setup time by the setup
command is not affected, you must copy them again by hand since they were
copied from an older version at download time. So this change will only
affects future installations. Another possibility to synchronize with the
production version is to download a new version of the
\verb|setup_pips.sh| script (or better make a link to
\verb|MYPIPS/prod/pips/makes/setup_pips.sh| for example to have an always
up-to-date version) and run again \verb|setup_pips.sh|. It won't erase the
directories but will pull the new version of subversion and will
reconstruct the \texttt{pipsrc.*sh} files.


\subsubsection{In directory \texttt{src/Documentation/pipsmake}}
\label{sec:direct-pipsmake}

\begin{itemize}
\item declare the new phase to \texttt{pipsmake} by adding the necessary
  rules in the \texttt{pipsmake-rc.tex} file. Each rule describes the
  dependences between the input and output resources of each phase. Just
  have a look at the other rules to build your owns. You can also add
  aliases to obtain nice menus in the \texttt{wpips} interface.

  For instance, for a phase \emph{myphase} which reads a resource
  \emph{res1} and produces a resource \emph{res2} for the current module,
  the rule and its alias are:
\begin{verbatim}
alias myphase 'My Phase'

myphase > MODULE.res2
        < PROGRAM.entities
        < MODULE.res1
\end{verbatim}

  Note that libraries generally use the program entities (for example if
  you display some code in debug routines...), that is to say all the
  objects from the current Fortran program and which have a name.  This
  resource is named \texttt{PROGRAM.entities}.

\item Run the command \texttt{make} to derive the various files
  \texttt{pipsmake.rc}, \texttt{phases.h}, \texttt{resources.h},
  \texttt{wpips.rc} and \texttt{builder-map.h} , and to copy them to
  \verb|$PIPS_ROOT/doc|, \verb|$PIPS_ROOT/include| and
  \verb|$PIPS_ROOT/etc|.
\end{itemize}


\subsubsection{In directory \texttt{src/Documentation/properties}}
\label{sec:srcd}

New \emph{properties} (that are global variables with a user interface in
PIPS) may be necessary for the fine control of the new phase.  They must
be declared in the \texttt{properties-rc.tex} file.

Use \texttt{make} to push them into PIPS.


\subsubsection{In directory \texttt{src/Libs/pipsmake}}
\label{sec:direct-lib-pipsmake}

Execute the command \texttt{make recompile} to recompile the
\texttt{pipsmake} library.


\subsubsection{In directory \texttt{src/Passes}}
\label{sec:direct-passes}

Execute the command \texttt{make recompile} to recompile and install
updated versions of \texttt{pips}, \texttt{tpips} and \texttt{wpips}.


\subsubsection{The final touch}
\label{sec:final-touch}

It can also be necessary to update the shell scripts \texttt{Init},
\texttt{Build}, \texttt{Select}, \texttt{Perform}, and \texttt{Display} in
directory \texttt{src/Scripts/drivers}.\marginpar{Are they still in use
  since tpips? Should be automatically generated anyway... RK}


\subsection{Dealing with a new resource}
\label{sec:dealing-with-new}

A new resource is declared by its use or its production by one of the rules
in the file {\tt pipsmake-rc.tex} (see below). Here are now the steps to
follow to deal with a new resource {\tt myres} once the phase {\tt myphase}
from the library {\tt mylib} has been declared and installed as shown in the
previous section, that is to say that the function
\begin{verbatim}
bool myphase(char *module_name)
\end{verbatim}
is available.

\subsubsection{In directory \texttt{src/Documentation/pipsmake}}
\label{sec:direct-pipsmake2}
  
  \begin{itemize}
    
  \item Declare the new resource \texttt{myres} by modifying the file 
    \texttt{pipsmake-rc.tex}. Each rule describes the
    dependances between the input and output resources of each phase. Just
    have a look at the other rules to build your owns. You can also add
    aliases to obtain nice menus in the \textbf{wpips} interface.
    
    For instance, for a phase \emph{myphase} which reads a resource 
    \texttt{another\_res} and produces a resource \emph{my res} for the 
    current module, the rule and its alias are:

\begin{verbatim}
alias myphase 'My Phase'

myphase > MODULE.myres
        < PROGRAM.entities
        < MODULE.another_res
\end{verbatim}

  \item Then you can do as the end of section~\ref{sec:direct-pipsmake}.
  
  \end{itemize}
  
\subsubsection{In the directory \texttt{src/Libs/pipsdbm}}
\label{sec:direct-pipsdbm}

The new resource (if any) must be declared to the resource manager
\texttt{pipsdbm}:
\begin{itemize}
\item Update the file \texttt{methods.h} by adding a line for the
  \texttt{DBR\_MYRES} resource. Macros are predefined for saving, loading,
  freeing and checking a resource if it is a newgen domain, a string, and
  so on. For instance a NewGen resource is simply declared as:
\begin{verbatim}
{ DBR_MYRES, NEWGEN_METHODS },
\end{verbatim}
  For file resources, the \texttt{pipsdbm} data usually is a string
  representing the file name within the pips database. All such resources
  are suffixed with \texttt{\_FILE}. It is declared as:
\begin{verbatim}
{ DBR_MYRES_FILE, STRING_METHODS },
\end{verbatim}

\item Run the commands \texttt{make} to update \texttt{pipsdbm} and
  install it.
\end{itemize}


\subsubsection{The final touch}
\label{sec:final-touch2}

Then follow with the end of section~\ref{sec:direct-pipsmake} and then
section~\ref{sec:direct-passes}.

\subsubsection{Remark}
\label{sec:remark}

It is necessary to recompile PIPS before any test, because changing the
header files generated from \texttt{pipsmake-rc.tex} can lead to
inconsistencies which only appear at run time (the database cannot be
created for instance).


\subsection{Modification or addition of a new NewGen data structure}

NewGen data strutures for PIPS are defined in \LaTeX{} files which are in
the directory \verb+$PIPS_ROOT/src/Documentation/newgen+.  These files
include both the NewGen specifications, and the comments in \LaTeX{}
format.  DDL\footnote{Newgen types are called \emph{domains\/} and are
  defined using a high level specification language called DDL for
  \emph{Domain Definition Language}.} files are automatically generated
from the previous \LaTeX{} files. When declaring a new data structure, two
cases may arise:

\begin{enumerate}

\item The new data structure is defined in a \emph{new} file
  \texttt{mysd.tex}:

  This file has to be declared in the local {\tt Makefile} (in
  \verb+$PIPS_ROOT/src/Documentation/newgen+) for its further installation
  in the PIPS hierarchy. It merely consists in adding its name
  \texttt{mysd.tex} to the \texttt{F.tex} variable list.

  If the data structure is to be managed by \texttt{pipsdbm}, \emph{i.e.}
  it is declared as a resource in \texttt{pipsmake-rc.tex}, then
\begin{verbatim}
$PIPS_ROOT/src/Lib/pipsdbm/methods.h
\end{verbatim}
  must be updated.

  It is not compulsory to immediately execute the command \texttt{make} in
  these directories. But it will be necessary before any global
  recompilation of PIPS (see below).

\item The new data structure is added to an already existing file
  \texttt{mysd.tex}: modifying this file is sufficient, and the above
  steps are not necessary.
\end{enumerate}

Then, in both cases, the next steps have to be performed:
\begin{itemize}

\item In the directory \verb+$PIPS_ROOT/src/Documentation/newgen+, %$
  execute the command \texttt{make}.  This command builds the DDL files
  \texttt{mysd.newgen}, \texttt{mysd.spec}, \texttt{mysd.h},
  \texttt{mysd.c} and some common files such as \texttt{specs.h} and this
  copies the generated files to the \verb+$PIPS_ROOT/include+ directory;

\item NewGen globally handles all the data structures defined in the
  project. Hence any minor modification of one of them, or any addition,
  potentially leads to a modification of all the interfaces which are
  generated. It is thus necessary to recompile everything in the
  production directory. It will also recompile
  \verb+$PIPS_ROOT/src/Documentation/newgen+ and
  \verb|$PIPS_ROOT/src/Lib/pipsdbm/methods.h| if they have been modified
  in a previous step. To recompile PIPS, see
  section~\ref{sec:building-pips}.
\end{itemize}


\subsection{Global variables, modifications}

Global variables should be avoided, or at least carefully documented to
avoid disturbing already existing programs. Properties (variables with a
user interface) may be used instead.

If new functionalities are required, it is better to keep the existing
module as such, and to create a new one, with another name.

For global variables, initialization, access and reset routines must be
provided. Do not forget that there may be several successive requests with
\verb+wpips+ or \verb+tpips+, whereas tests performs with shell interfaces
(for instance \verb+Build+) are much simpler.


\newpage
\noindent
{\huge MAY BE OBSOLETE DOWN FROM HERE\ldots}

\section{Development (\texttt{PIPS\_DEVEDIR})}

This directory contains the version of PIPS currently being developed.  It
is a mirror of the organization under \verb+$PIPS_ROOT/src+. %%$
Each library is developed and tested under this subtree, linking by
default with the \emph{installed} libraries in \verb+$PIPS_ROOT/Lib+. %%$

Thus new versions can be developed without interfering with other
developments. Once a new version of a library (or pass, or script, or
anything) is okay and validated, it is installed under the
\verb+$PIPS_ROOT/Src+ subtree and becomes the reference for all other
developers who may use its services. %%$

When developing or debugging several related libraries at the same time,
you can used the development version of these libraries by using unix
links (\texttt{ln -s}) to one of the directory under the corresponding
\verb|$PIPS_ARCH| subdirectory. When building %%$
a new binary, the linker takes these local libraries first.

\subsection{Experiments}

At CRI, this directory is a large piece of disk space to be used
temporarily to analyze large programs and to core dump large {\tt pips}
processes; no permanent, non-deductible information should be stored
there.


\section{Linear library}
\label{sec:linear}

The linear library is also independent from PIPS. Its development was funded
by the French organization CNRS.


Its root directory is \verb+/projects/C3/Linear/+
(\verb+$LINEAR_DIR+). This directory is further divided into
similarily to Pips and Newgen.  Thus there are Production
(\verb+$LINEAR_ROOT+) and Development directories.
\verb+$LINEAR_ARCH+ (which defaults to \verb+$PIPS_ARCH+ and \verb|.|
(dot)) can be used for building the library.


\section{Organization of a PIPS pass}
\label{sec:pass_organization}


The source files for a pass {\tt p} can be found in the directory {\tt
  Development/Passes/p}. A pass, as opposed to a library, corresponds to an
  executable, which can be used by the users. 

\textbf{TODO} : update this section since config.makefile does no longer
exist !

In this directory, a local {\tt config.makefile} must be created. It must be
updated with the names of the source files. Then the local {\tt Makefile}
can be automatically derived using {\tt pips-makemake -p}. This {\tt
  Makefile} contains among others an {\tt install} entry to install the pass
in the {\bf\tt Production} hierarchy. 

The libraries used to build the pass are in the directories {\tt
  Production/Libs} and {\tt Externals}. The corresponding header files are
in {\tt Production/Include} and {\tt Externals}.

To maximize code reuse, it is recommended to limit code development at this
level to functionalities really specific to passes. Everything else should
be placed in libraries (for instance in {\tt top-level}).

At the library level, it is also possible to create local executables by
invoking {\tt make test}, {\tt make ttest} or {\tt make wtest}. 

If a pass and a library are simultaneously developed, {\tt misc} and {\tt
  parallelize} for instance, one of the library must be chosen for the link,
{\tt parallelize} for instance). It is then necessary to look for {\tt
  misc.h} and {\tt libmisc.a} in {\tt Development/Lib/misc} instead of {\tt
  Production/Libs/misc}. This can be done very simply in the
{config.makefile} file by giving the names of the libraries to use:
\begin{verbatim}
...
# List of libraries used to build the target
TARGET_LIBS=    -lprivatize -lusedef -lprettyprint -lsemantics \
                -ltransformer \
                -lcontrol -leffects -lnormalize \
                -lsc -lcontrainte -lvecteur -larithmetique \
                -lri-util ../../Libs/libmisc.a \
                -lproperties -lgenC /usr/lib/debug/malloc.o \
                -lproperties

$(TARGET): ../Lib/misc/misc.a
\end{verbatim}

\section{Conventions}
\label{sec:conventions}


libraries are named \verb|libXXX.a| where \verb|XXX| is the logical name of
the library: {\tt vecteur}, {\tt prettyprint}, {\tt misc},etc.

It is theoretically unuseful to put libraries in the Makefile dependencies,
because header files which are automatically generated are in these
dependencies, and are systematically modified each time an installation is
performed. However, this does not work if the pass {\tt p} calls the library
{\tt a} which needs the library {\tt b}, and if {\tt p} does not directly
needs the library {\tt b}: modifications to {\tt b} will not provoke the
link of {\tt p}. 

Each important library uses an environment variable \verb+XXX_DEBUG_LEVEL+
to control debugging messages. This rule hase an exception:
\verb+PARSER_DEBUG_LEVEL+ corresponds to the \verb+syntax+ library .

Level 0 corresponds to the normal behaviour.
The highest level is 8. Great care has to be brought to calls to
\verb+debug_on()+ and \verb+debug_off()+, because successive debug levels
are stored in a stack, and it could disturb its coherency. The debug of a
function of another library should not unconsciously be activated.  

\section{Bug policy}
\label{sec:bugs}

\subsection{Bug detection}

When a bug has been detected, a small Fortran program  ({\tt bug.f} for
instance) must be written to reproduce the bug. 

If the library {\tt xxx} is responsible for this bug, move bug.f in {\tt
  Tests/Bugs/xxx/}. This responsibility can be found using
\verb+XXX_DEBUG_LEVEL+ or a debugging tool.

Then record the new bug in {\tt \$PIPS\_DOCDIR/pips-bugs.f.tex}.

\subsection{Bug correction}

Go to the directory {\tt /Tests/Bugs/xxx}. In order to use the executable
of the library {\tt xxx}, create a symbolic link towards it ({\tt ln -s
  Development/Lib/xxx/pips} for instance).  Here are now the different steps
to perform:

\begin{itemize}
\item Find the sources responsible for the bugs and correct them (we assume
  that only one library is responsible for the bug; the case of multiple
  libraries is presented below). 

\item Run \verb+make test+ in {\tt Development/Lib/xxx} to build an executable.

\item Back in {\tt Tests/Bugs/xxx}, run:
\begin{verbatim}
Init -f bug.f bug
Display -m -v bug
\end{verbatim}
and verify that the bug has been eliminated.

\item In {\tt Development/Lib/xxx/}:
  \begin{itemize}
  \item Launch \verb+Validate Xxx+ to run the non regression tests specific
    to the library {\tt xxx}.
    
  \item Run \verb+Validate+ if the changes may affect the results in several
    phases.
    
  \item If the results are satisfactory, run \verb+make install+ to install
    the new sources in the {\bf\tt Production} hierarchy. Beware that this
    does not build new executables in {\tt \$PIPS\_BINDIR}. This can be done
    with the {\tt make-pips} command, but this is not compulsory, because
    this command is automatically run each night.
  \end{itemize}
\end{itemize}

Once these operations have been done, the program {\tt bug.f} and the
results obtained for its parallelization must be added
to the non-regresson tests in the directory {\tt /Tests/Validation/Xxx}.
For that purpose, you can run the command \verb+bug-to-validate bug.f+.
But beware that this command is a script shell which provides a mere
parallelization of {\tt bug.f}. To transfer all the programs from the
directory {\tt Tests/Bugs/xxx} to {\tt /Tests/Validation/Xxx}, you can use the
command \verb+dir-to-validate+. Again, this is very convenient when the
desired test is the default test (see
Section~\ref{subsec:the_validation_directory}). 

\paragraph{Remark:} When several libraries are responsible for the bug, say
{\tt xxx1},\dots, {\tt xxxk}, chose one of the libraries to build the
executable by linking with the other libraries. Once the problems are fixed,
do not forget to run {\tt make install} in all the libraries.

\subsection{The {\bf \tt Validation} directories}
\label{subsec:the_validation_directory}

\textsc{TODO} : update this section to the valid svn directory.

These directories {\tt /Tests/Validation/Xxx} contain the non-regression
tests for each significant library {\tt xxx}, as well as demonstration
programs, benchmarks, \dots.

For each Fortran file \verb+bug.f+, there exists a test file
\verb+bug.test+, and a sub-directory \verb+bug.result+ in which the results
are stored in a \verb+test+ file. All these files can be generated by
\verb+bug-to-validate bug.f+ when dealing with a bug in {\tt
  Tests/Bugs/xxx}. If this command has been used, the reuslt directory
contains the results of a mere parallelization. For more complex results,
a specific test file must be written, as well as a script inspired from
\verb+bug-to-validate+ to install the necessary files in the validation
directory. 

If the same test file is valid for a whole directory (ex : {\tt
  Validation/Flint}), a generic file \verb+default_test+ can be created in
this directory. In this file, the generic names for the program are
\verb+tested_file+ and \verb+TESTED_FILE+. Here is the {\tt default\_test}
script of \verb+Validation/Flint+:
\begin{verbatim}
#!/bin/sh
Init -d -f $PIPSDIR/Tests/Validation/Flint/tested_file.f \
           tested_file 2>/dev/null >/dev/null
Perform -m tested_file flinter 2>/dev/null 
cat tested_file.database/TESTED_FILE.flinted 
Delete tested_file 2>/dev/null
\end{verbatim}
Notice that if there exists a local test for the file ({\tt bug.test}), it
will be used first. The priority starts from local files to general ones. 

\subsection{Other validation}

The command \verb+Validate+ can be used to measure the impact of a
modification by comparing the new behavior of PIPS with the preceding one. 

To use it to check the parallelization process, put your test file, say
{\tt mytest.f} which contains the main program MAIN and a subroutine SUB, into
one of the directories in
\verb+~pips/Pips/Tests/Validation+ directory. You can also create your own
directory there if you want to ensure that a particular aspect of PIPS
behaves the correct way.

Once {\tt mytest.f} is in such a directory, say {\tt kludge}, you should do
the following thing.
\begin{verbatim}
Validation/kludge: Init -f mytest.f mytest
Validation/kludge: mkdir mytest.result
Validation/kludge: Display -m main > mytest.result/MAIN
Validation/kludge: Display -m sub > mytest.result/SUB
Validation/kludge: Delete mytest
\end{verbatim}

Check that the output in the MODULE1, MODULE2, ... files is what you
want ... and that's it!

After a while, if you want to check that PIPS still does that it was
supposed to do, go into Validation and type

\begin{verbatim}
Validate kludge
\end{verbatim}

If there is any problem, you will get a mail message that tells you want the
problem is. You can also type {\tt Validate} to check everything.

\begin{itemize}
\item \verb+Validate+ with no argument validates all the directories which
  are listed in
  the file {\tt Validation/defaults}.
  
\item When a directory of {\tt Validation} is validated, if for the program
  {\tt foo.f} there exists a file {\tt foo.test} it is executed; otherwise
  {\tt default\_test} is ran. The output is compared to {\tt foo.result/test}
  which must have been created before. The result can be as complex as
  desired, and not a mere parallelized version.

\item {\tt tpips} scripts can also be used. In this case, do not forget to
  build a local {\tt tpips} by invoking {\tt make ttest} after creating your
  local {\tt pips}.
\end{itemize}



\section{Miscellaneous}
\label{sec:misc}

To print a nice version of PIPS output, you can use commands like
\verb+tgrind+:
\begin{verbatim}
tgrind -lf extr_doall.f
\end{verbatim}

All the source of PIPS can be pretty-printed with Doxygen and are
browsable. For example, see \LINK{http://doxygen.pips.enstb.org}.

\subsection{Changing the dynamic allocation library}

Errors which are the most difficult to find generally are dynamic allocation
errors: The allocated space is accesssed after being freed ({\em
  dangling pointer}), or a zone larger than the allocated zone is
referenced (too long copy of a character string for instance).

A first checking level is available with a set of SUN primitives ({\tt
  man malloc.h}), by linking the program with
\verb+/usr/lib/debug/malloc.o+. The choice of the dynamic allocation library
can be made in \verb+pipsrc.ref+, or in the curretn environment, but without
any guarantee for the link.

A second level, a lot more efficient, but also much slower, is offerd by a
public domain library, \verb+malloclib-pl11+. This library systematically
overwrites freed zones with a special pattern, and checks that string
operations are valid. It also records in which files allocations and
desallocations are performed.

To use it efficiently, all the source files which dynamically allocate or
free memory must include \verb+malloc.h+ with double quotes (and not $< \;
>$ signs). This is achieved in most cases because NewGen includes
\verb+malloc.h+. But some files, such as character strings manipulation
files, do not include {\tt GenC.h}: {\tt \#include "malloc.h"} must be added
to them.

PIPS must then be entirely recompiled (see section~\ref{sec:building-pips}), as
well as the libraries in {\tt \$PIPS\_EXTEDIR}, after renaming ({\tt mv})
\verb+dbmalloc.h+ into \verb+malloc.h+ in {\tt \$PIPS\_EXTEDIR} and in the
directories containing the sources of the external libraries (NewGen and
Linear). Before linking, the environment variable \verb+PIPS_LIBS+ must
have been modified, either in the current environment, or in
\verb+pipsrc.ref+, to include the correct library.

It is recommended to keep an version of PIPS linkeed with a reasonnably fast
library to be able to create large databases for the tests. To avoid
compiling and linking too many things each time, it is also recommended to
modify the shell variables \verb+$PIPS_BINDIR+ and \verb+$PIPS_LIBDIR+ to
keep both versions of PIPS.



\newpage


{\small
\bibliographystyle{plain}
\bibliography{developer_guide}
}

\printindex

\end{document}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% ispell-local-dictionary: "american"
%%% End: 
